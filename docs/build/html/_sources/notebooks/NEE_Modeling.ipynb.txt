{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA,KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib as mpl\n",
    "import seaborn as sb\n",
    "\n",
    "file = pd.ExcelFile('Data_Quanterra.xls') # File conversion multiple to -->single .csv or .json\n",
    "file.sheet_names\n",
    "\n",
    "\n",
    "# Read individual sheets\n",
    "df_HurstWest = pd.read_excel(file,file.sheet_names[0], skiprows=25)\n",
    "df_HurstEast = pd.read_excel(file,file.sheet_names[1], skiprows=25)\n",
    "df_FergusonSW = pd.read_excel(file,file.sheet_names[2], skiprows=25)\n",
    "df_FergusonNE = pd.read_excel(file,file.sheet_names[3], skiprows=25)\n",
    "\n",
    "# Save units separately\n",
    "units = df_HurstWest.iloc[0]\n",
    "\n",
    "# Drop the units row\n",
    "df_HurstWest.drop([0],inplace=True)\n",
    "df_HurstEast.drop([0],inplace=True)\n",
    "df_FergusonSW.drop([0],inplace=True)\n",
    "df_FergusonNE.drop([0],inplace=True)\n",
    "\n",
    "\n",
    "df_HurstWest.head()\n",
    "\n",
    "\n",
    "\n",
    "HE = pd.read_parquet('HurstEast.parquet')\n",
    "HW = pd.read_parquet('HurstWest.parquet')\n",
    "FNE = pd.read_parquet('FergusonNE.parquet')\n",
    "FSW = pd.read_parquet('FergusonSW.parquet')\n",
    "\n",
    "\n",
    "HE['RAINFALL_14DAYS_SUM'] = HE['RAINFALL'].rolling(window=336*2, min_periods=1).sum()\n",
    "HW['RAINFALL_14DAYS_SUM'] = HW['RAINFALL'].rolling(window=336*2, min_periods=1).sum()\n",
    "FNE['RAINFALL_14DAYS_SUM'] = FNE['RAINFALL'].rolling(window=336*2, min_periods=1).sum()\n",
    "FSW['RAINFALL_14DAYS_SUM'] = FSW['RAINFALL'].rolling(window=336*2, min_periods=1).sum()\n",
    "\n",
    "\n",
    "HE['RAINFALL_7DAYS_SUM'] = HE['RAINFALL'].rolling(window=336, min_periods=1).sum()\n",
    "HW['RAINFALL_7DAYS_SUM'] = HW['RAINFALL'].rolling(window=336, min_periods=1).sum()\n",
    "FNE['RAINFALL_7DAYS_SUM'] = FNE['RAINFALL'].rolling(window=336, min_periods=1).sum()\n",
    "FSW['RAINFALL_7DAYS_SUM'] = FSW['RAINFALL'].rolling(window=336, min_periods=1).sum()\n",
    "\n",
    "\n",
    "Names = ['HurstEast','HurstWest','FergusonNE','FergusonSW']\n",
    "DFS = [HE, HW, FNE, FSW]\n",
    "\n",
    "date_from = FNE['MIDPOINT_DATETIME'].iloc[0]\n",
    "date_to = FNE['MIDPOINT_DATETIME'].iloc[-1]\n",
    "\n",
    "file = pd.ExcelFile('Data_Quanterra.xls') # File conversion multiple to -->single .csv or .json\n",
    "df_HurstWest = pd.read_excel(file,file.sheet_names[0], skiprows=25)\n",
    "\n",
    "# Save units separately\n",
    "units = df_HurstWest.iloc[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_doy(DF):\n",
    "    DF_FNE = DF.copy(deep=True)\n",
    "    DF_FNE['DOY'] = DF_FNE['MIDPOINT_DATETIME'].dt.dayofyear  # Directly extract DOY\n",
    "    DF_FNE['DOY_sin'] = np.sin(2 * np.pi * DF_FNE['DOY'] / 365)\n",
    "    DF_FNE['DOY_cos'] = np.cos(2 * np.pi * DF_FNE['DOY'] / 365)\n",
    "    return DF_FNE\n",
    "\n",
    "def add_cos_sin_direction(DF):\n",
    "    DF_FNE = DF.copy(deep=True)\n",
    "    DF_FNE['cos_theta'] = np.cos(np.pi * DF_FNE['WIND_DIRECTION'] / 180)\n",
    "    DF_FNE['sin_theta'] = np.sin(np.pi * DF_FNE['WIND_DIRECTION'] / 180)\n",
    "    return DF_FNE\n",
    "\n",
    "\n",
    "DF_FNE = add_doy(FNE)\n",
    "DF_FSW = add_doy(FSW)\n",
    "DF_HE = add_doy(HE)\n",
    "DF_HW = add_doy(HW)\n",
    "\n",
    "\n",
    "DF_FNE = add_cos_sin_direction(DF_FNE)\n",
    "DF_FSW = add_cos_sin_direction(DF_FSW)\n",
    "DF_HE = add_cos_sin_direction(DF_HE)\n",
    "DF_HW = add_cos_sin_direction(DF_HW)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preds_after_corr = ['INCOMING_SHORTWAVE_RADIATION',\n",
    "#  'AIR_PRESSURE',\n",
    "#  'VAPOUR_PRESSURE_DEFICIT',\n",
    "#  'SOIL_MOISTURE',\n",
    "#  'SOIL_TEMPERATURE',\n",
    "#  'DOY_sin',\n",
    "#  'DOY_cos',\n",
    "#  'RAINFALL_14DAYS_SUM']\n",
    "\n",
    "\n",
    "preds_after_corr = ['INCOMING_SHORTWAVE_RADIATION',\n",
    " 'AIR_PRESSURE',\n",
    " 'VAPOUR_PRESSURE_DEFICIT',\n",
    " 'SOIL_MOISTURE',\n",
    " 'SOIL_TEMPERATURE',\n",
    " 'AIR_TEMPERATURE',\n",
    " 'SOIL_HEAT_FLUX',\n",
    " 'DOY_sin',\n",
    " 'DOY_cos',\n",
    " 'RAINFALL_14DAYS_SUM',\n",
    " 'WIND_SPEED',\n",
    " 'sin_theta',\n",
    " 'cos_theta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = DF_FSW.copy()\n",
    "pred_MAT = DF[DF['NET_CARBON_DIOXIDE_FLUX_FLAG']==0][preds_after_corr] #Only considering the non-imputed flux\n",
    "target_var = DF[DF['NET_CARBON_DIOXIDE_FLUX_FLAG']==0]['NET_CARBON_DIOXIDE_FLUX']\n",
    "\n",
    "pred_MAT_FNE = DF[DF['NET_CARBON_DIOXIDE_FLUX_FLAG']==0][preds_after_corr] #Only considering the non-imputed flux\n",
    "target_var_FNE = DF[DF['NET_CARBON_DIOXIDE_FLUX_FLAG']==0]['NET_CARBON_DIOXIDE_FLUX']\n",
    "\n",
    "pred_MAT_FSW = DF_FSW[DF_FSW['NET_CARBON_DIOXIDE_FLUX_FLAG']==0][preds_after_corr] #Only considering the non-imputed flux\n",
    "target_var_FSW = DF_FSW[DF_FSW['NET_CARBON_DIOXIDE_FLUX_FLAG']==0]['NET_CARBON_DIOXIDE_FLUX']\n",
    "\n",
    "pred_MAT_HE = DF_HE[DF_HE['NET_CARBON_DIOXIDE_FLUX_FLAG']==0][preds_after_corr] #Only considering the non-imputed flux\n",
    "target_var_HE = DF_HE[DF_HE['NET_CARBON_DIOXIDE_FLUX_FLAG']==0]['NET_CARBON_DIOXIDE_FLUX']\n",
    "\n",
    "pred_MAT_HW = DF_HW[DF_HW['NET_CARBON_DIOXIDE_FLUX_FLAG']==0][preds_after_corr] #Only considering the non-imputed flux\n",
    "target_var_HW = DF_HW[DF_HW['NET_CARBON_DIOXIDE_FLUX_FLAG']==0]['NET_CARBON_DIOXIDE_FLUX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF['tan_theta'] = DF['sin_theta']/DF['cos_theta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directional Influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DF_FSW_subset = DF_FSW[['MIDPOINT_DATETIME', 'WIND_SPEED', 'WIND_DIRECTION']].copy(deep=True)\n",
    "DF_FSW_subset['MIDPOINT_DATETIME'] = pd.to_datetime(DF_FSW_subset['MIDPOINT_DATETIME'])\n",
    "DF_FSW_2023 = DF_FSW_subset[DF_FSW_subset['MIDPOINT_DATETIME'].dt.year >= 2023]\n",
    "\n",
    "num_bins = 36\n",
    "bin_edges = np.linspace(0, 360, num_bins + 1)\n",
    "bin_centers = np.radians((bin_edges[:-1] + bin_edges[1:]) / 2)\n",
    "\n",
    "DF_FSW_subset['wind_direction_rad'] = np.radians(DF_FSW_subset['WIND_DIRECTION'])\n",
    "\n",
    "# Bin the wind directions\n",
    "DF_FSW_subset['wind_bin'] = pd.cut(DF_FSW_subset['WIND_DIRECTION'], bins=bin_edges, include_lowest=True)\n",
    "\n",
    "# Count the frequency of wind directions in each bin\n",
    "bin_counts = DF_FSW_subset['wind_bin'].value_counts().sort_index()\n",
    "\n",
    "# Get center of each bin in radians\n",
    "bin_centers_deg = [(interval.left + interval.right) / 2 for interval in bin_counts.index]\n",
    "bin_centers_rad = np.radians(bin_centers_deg)\n",
    "\n",
    "# Plotting the polar histogram with a concentric circle at the center\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, polar=True)\n",
    "\n",
    "bars = ax.bar(\n",
    "    bin_centers_rad,\n",
    "    bin_counts.values,\n",
    "    width=np.radians(360 / num_bins),\n",
    "    align='center',\n",
    "    edgecolor='k',\n",
    "    bottom=100  # <-- Starts the bars from radius = 1 instead of 0\n",
    ")\n",
    "\n",
    "ax.set_theta_zero_location(\"N\")  # 0 degrees at the top (North)\n",
    "ax.set_theta_direction(-1)       # Clockwise direction (like a compass)\n",
    "\n",
    "ax.set_title(\"FSW Wind Direction Histogram\", va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make a copy of relevant columns\n",
    "DF_FSW_subset = DF_FSW[['MIDPOINT_DATETIME', 'WIND_SPEED', 'WIND_DIRECTION']].copy(deep=True)\n",
    "DF_FSW_subset['MIDPOINT_DATETIME'] = pd.to_datetime(DF_FSW_subset['MIDPOINT_DATETIME'])\n",
    "\n",
    "# Filter year >= 2023\n",
    "DF_FSW_2023 = DF_FSW_subset[DF_FSW_subset['MIDPOINT_DATETIME'].dt.year >= 2023]\n",
    "\n",
    "# Wind direction bins\n",
    "num_dir_bins = 36\n",
    "dir_bin_edges = np.linspace(0, 360, num_dir_bins + 1)\n",
    "dir_bin_centers_deg = (dir_bin_edges[:-1] + dir_bin_edges[1:]) / 2\n",
    "dir_bin_centers_rad = np.radians(dir_bin_centers_deg)\n",
    "\n",
    "# Wind speed bins\n",
    "speed_bins = [0, 2, 4, 6, 10, np.inf]\n",
    "speed_labels = ['0-2', '2-4', '4-6', '6-10', '10+']\n",
    "DF_FSW_subset['speed_bin'] = pd.cut(DF_FSW_subset['WIND_SPEED'], bins=speed_bins, labels=speed_labels, include_lowest=True)\n",
    "\n",
    "# Assign wind direction bins\n",
    "DF_FSW_subset['dir_bin'] = pd.cut(DF_FSW_subset['WIND_DIRECTION'], bins=dir_bin_edges, labels=False, include_lowest=True)\n",
    "\n",
    "# Create pivot table for stacked bar heights\n",
    "windrose_data = DF_FSW_subset.pivot_table(index='dir_bin', columns='speed_bin', aggfunc='size', fill_value=0)\n",
    "windrose_data = windrose_data.reindex(range(num_dir_bins), fill_value=0)\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, polar=True)\n",
    "\n",
    "bar_width = np.radians(360 / num_dir_bins)\n",
    "base_radius = 100  # <-- Radius from which bars start (concentric base circle)\n",
    "bottoms = np.full(num_dir_bins, base_radius)\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(speed_labels)))  # Colormap for stacking\n",
    "\n",
    "# Stack bars for each wind speed bin\n",
    "for i, speed_label in enumerate(speed_labels):\n",
    "    heights = windrose_data[speed_label].values\n",
    "    ax.bar(\n",
    "        dir_bin_centers_rad,\n",
    "        heights,\n",
    "        width=bar_width,\n",
    "        bottom=bottoms,\n",
    "        color=colors[i],\n",
    "        edgecolor='black',\n",
    "        label=speed_label\n",
    "    )\n",
    "    bottoms += heights  # Stack next layer on top\n",
    "\n",
    "# Polar axis settings\n",
    "ax.set_theta_zero_location(\"N\")\n",
    "ax.set_theta_direction(-1)\n",
    "\n",
    "# Optional: Hide radial ticks below base radius\n",
    "ax.set_rmin(0)\n",
    "ax.set_rticks([])\n",
    "\n",
    "# Title and legend\n",
    "ax.set_title(\"FSW Wind Direction Histogram (Stacked by Wind Speed)\", va='bottom')\n",
    "ax.legend(title='Wind Speed (m/s)', bbox_to_anchor=(1.1, 1.0))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make a copy of relevant columns\n",
    "DF_FSW_subset = DF_HE[['MIDPOINT_DATETIME', 'WIND_SPEED', 'WIND_DIRECTION']].copy(deep=True)\n",
    "DF_FSW_subset['MIDPOINT_DATETIME'] = pd.to_datetime(DF_FSW_subset['MIDPOINT_DATETIME'])\n",
    "\n",
    "# Filter year >= 2023\n",
    "DF_FSW_2023 = DF_FSW_subset[DF_FSW_subset['MIDPOINT_DATETIME'].dt.year >= 2023]\n",
    "\n",
    "# Wind direction bins\n",
    "num_dir_bins = 36\n",
    "dir_bin_edges = np.linspace(0, 360, num_dir_bins + 1)\n",
    "dir_bin_centers_deg = (dir_bin_edges[:-1] + dir_bin_edges[1:]) / 2\n",
    "dir_bin_centers_rad = np.radians(dir_bin_centers_deg)\n",
    "\n",
    "# Wind speed bins\n",
    "speed_bins = [0, 2, 4, 6, 10, np.inf]\n",
    "speed_labels = ['0-2', '2-4', '4-6', '6-10', '10+']\n",
    "DF_FSW_subset['speed_bin'] = pd.cut(DF_FSW_subset['WIND_SPEED'], bins=speed_bins, labels=speed_labels, include_lowest=True)\n",
    "\n",
    "# Assign wind direction bins\n",
    "DF_FSW_subset['dir_bin'] = pd.cut(DF_FSW_subset['WIND_DIRECTION'], bins=dir_bin_edges, labels=False, include_lowest=True)\n",
    "\n",
    "# Create pivot table for stacked bar heights\n",
    "windrose_data = DF_FSW_subset.pivot_table(index='dir_bin', columns='speed_bin', aggfunc='size', fill_value=0)\n",
    "windrose_data = windrose_data.reindex(range(num_dir_bins), fill_value=0)\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, polar=True)\n",
    "\n",
    "bar_width = np.radians(360 / num_dir_bins)\n",
    "base_radius = 100  # <-- Radius from which bars start (concentric base circle)\n",
    "bottoms = np.full(num_dir_bins, base_radius)\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(speed_labels)))  # Colormap for stacking\n",
    "\n",
    "# Stack bars for each wind speed bin\n",
    "for i, speed_label in enumerate(speed_labels):\n",
    "    heights = windrose_data[speed_label].values\n",
    "    ax.bar(\n",
    "        dir_bin_centers_rad,\n",
    "        heights,\n",
    "        width=bar_width,\n",
    "        bottom=bottoms,\n",
    "        color=colors[i],\n",
    "        edgecolor='black',\n",
    "        label=speed_label\n",
    "    )\n",
    "    bottoms += heights  # Stack next layer on top\n",
    "\n",
    "# Polar axis settings\n",
    "ax.set_theta_zero_location(\"N\")\n",
    "ax.set_theta_direction(-1)\n",
    "\n",
    "# Optional: Hide radial ticks below base radius\n",
    "ax.set_rmin(0)\n",
    "ax.set_rticks([])\n",
    "\n",
    "# Title and legend\n",
    "ax.set_title(\"HE Wind Direction Histogram (Stacked by Wind Speed)\", va='bottom')\n",
    "ax.legend(title='Wind Speed (m/s)', bbox_to_anchor=(1.1, 1.0))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DF_FSW_subset = DF_HW[['MIDPOINT_DATETIME', 'WIND_SPEED', 'WIND_DIRECTION']].copy(deep=True)\n",
    "DF_FSW_subset['MIDPOINT_DATETIME'] = pd.to_datetime(DF_FSW_subset['MIDPOINT_DATETIME'])\n",
    "DF_FSW_2023 = DF_FSW_subset[DF_FSW_subset['MIDPOINT_DATETIME'].dt.year >= 2023]\n",
    "\n",
    "num_bins = 36\n",
    "bin_edges = np.linspace(0, 360, num_bins + 1)\n",
    "bin_centers = np.radians((bin_edges[:-1] + bin_edges[1:]) / 2)\n",
    "\n",
    "DF_FSW_subset['wind_direction_rad'] = np.radians(DF_FSW_subset['WIND_DIRECTION'])\n",
    "\n",
    "# Bin the wind directions\n",
    "DF_FSW_subset['wind_bin'] = pd.cut(DF_FSW_subset['WIND_DIRECTION'], bins=bin_edges, include_lowest=True)\n",
    "\n",
    "# Count the frequency of wind directions in each bin\n",
    "bin_counts = DF_FSW_subset['wind_bin'].value_counts().sort_index()\n",
    "\n",
    "# Get center of each bin in radians\n",
    "bin_centers_deg = [(interval.left + interval.right) / 2 for interval in bin_counts.index]\n",
    "bin_centers_rad = np.radians(bin_centers_deg)\n",
    "\n",
    "# Plotting the polar histogram with a concentric circle at the center\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, polar=True)\n",
    "\n",
    "bars = ax.bar(\n",
    "    bin_centers_rad,\n",
    "    bin_counts.values,\n",
    "    width=np.radians(360 / num_bins),\n",
    "    align='center',\n",
    "    edgecolor='k',\n",
    "    bottom=100  # <-- Starts the bars from radius = 1 instead of 0\n",
    ")\n",
    "\n",
    "ax.set_theta_zero_location(\"N\")  # 0 degrees at the top (North)\n",
    "ax.set_theta_direction(-1)       # Clockwise direction (like a compass)\n",
    "\n",
    "ax.set_title(\"HW Wind Direction Histogram\", va='bottom')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT Shortwave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFS = [DF_FNE, DF_FSW, DF_HE, DF_HW]\n",
    "DFS[0][['MIDPOINT_DATETIME','NET_CARBON_DIOXIDE_FLUX','INCOMING_SHORTWAVE_RADIATION']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "\n",
    "# List of DataFrames\n",
    "DFS = [DF_FNE, DF_FSW, DF_HE, DF_HW]\n",
    "site_names = ['FergusonNE', 'FergusonSW', 'HurstEast', 'HurstWest']\n",
    "\n",
    "# Process each dataframe\n",
    "agg_dfs = []\n",
    "for df, site in zip(DFS, site_names):\n",
    "    df['DATE'] = df['MIDPOINT_DATETIME'].dt.date  # Extract date\n",
    "    daily_df = df.groupby('DATE').agg({\n",
    "        'NET_CARBON_DIOXIDE_FLUX': 'sum',  # Sum CO₂ flux\n",
    "        'INCOMING_SHORTWAVE_RADIATION': 'median'  # Median Shortwave Radiation\n",
    "    }).reset_index()\n",
    "    daily_df['SITE'] = site\n",
    "    daily_df['MONTH'] = pd.to_datetime(daily_df['DATE']).dt.month  # Extract month\n",
    "    agg_dfs.append(daily_df)\n",
    "\n",
    "# Combine all site data\n",
    "df_all = pd.concat(agg_dfs, ignore_index=True)\n",
    "\n",
    "# Determine color limits to center at 0\n",
    "vmax = df_all['NET_CARBON_DIOXIDE_FLUX'].abs().max()\n",
    "vmin = -vmax\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 8), sharex=True, sharey=True)\n",
    "fig.suptitle(\"Daily Aggregated Shortwave Radiation vs Month (Color: CO₂ Flux)\")\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, site in enumerate(site_names):\n",
    "    ax = axes[i]\n",
    "    site_df = df_all[df_all['SITE'] == site]\n",
    "    scatter = ax.scatter(site_df['MONTH'], site_df['INCOMING_SHORTWAVE_RADIATION'], \n",
    "                         c=site_df['NET_CARBON_DIOXIDE_FLUX'], cmap='coolwarm', alpha=0.6, vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(site)\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"INCOMING_SHORTWAVE_RADIATION (W/m²)\")\n",
    "\n",
    "# Colorbar\n",
    "cbar = fig.colorbar(scatter, ax=axes, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label(\"Net Carbon Dioxide Flux (μmol/m²/s)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_after_corr = ['INCOMING_SHORTWAVE_RADIATION',\n",
    "'OUTGOING_LONGWAVE_RADIATION',\n",
    " 'AIR_PRESSURE',\n",
    " 'VAPOUR_PRESSURE_DEFICIT',\n",
    " 'SOIL_MOISTURE',\n",
    " 'SOIL_TEMPERATURE',\n",
    " 'AIR_TEMPERATURE',\n",
    " 'SOIL_HEAT_FLUX',\n",
    " 'RAINFALL_14DAYS_SUM',\n",
    " 'WIND_SPEED',\n",
    " 'sin_theta',\n",
    " 'cos_theta',\n",
    " 'DOY',\n",
    " 'Hour_sin',\n",
    " 'Hour_cos',\n",
    " 'NDVI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = DFS[3].copy(deep=True)\n",
    "DF = DF[DF['MIDPOINT_DATETIME'].dt.year == 2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF['tan_theta'] = DF['sin_theta']/DF['cos_theta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "ndvi_mean = ndvi_hw['NDVI'].median(dim=['x', 'y'])\n",
    "\n",
    "DF = DF_HW.copy(deep=True)\n",
    "DF['NDVI'] = ndvi_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Assuming DF is your dataset\n",
    "DF['tan_theta'] = DF['sin_theta']/DF['cos_theta']\n",
    "X = DF[preds_after_corr]\n",
    "y = DF[\"NET_CARBON_DIOXIDE_FLUX\"]\n",
    "\n",
    "\n",
    "# Compute mutual information\n",
    "mi = mutual_info_regression(X, y, random_state=42)\n",
    "mi_series = pd.Series(mi, index=preds_after_corr).sort_values(ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "mi_series.plot(kind=\"bar\", color=\"royalblue\", edgecolor=\"black\")\n",
    "plt.xlabel(\"Predictors\")\n",
    "plt.ylabel(\"Mutual Information\")\n",
    "plt.title(\"Mutual Information between Predictors and CO2 Flux\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "preds_after_corr = ['INCOMING_SHORTWAVE_RADIATION',\n",
    "'OUTGOING_LONGWAVE_RADIATION',\n",
    " 'VAPOUR_PRESSURE_DEFICIT',\n",
    " 'SOIL_MOISTURE',\n",
    " 'SOIL_TEMPERATURE',\n",
    " 'AIR_TEMPERATURE',\n",
    " 'SOIL_HEAT_FLUX',\n",
    " 'RAINFALL_14DAYS_SUM',\n",
    " 'RAINFALL_7DAYS_SUM',\n",
    " 'WIND_SPEED',\n",
    " 'DOY',\n",
    " 'Hour_cos',\n",
    " 'NDVI']\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = DF[preds_after_corr + [\"NET_CARBON_DIOXIDE_FLUX\"]].corr()\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix with predictors having MI > 0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "preds_after_corr = ['INCOMING_SHORTWAVE_RADIATION',\n",
    "'OUTGOING_LONGWAVE_RADIATION',\n",
    " 'VAPOUR_PRESSURE_DEFICIT',\n",
    " 'SOIL_MOISTURE',\n",
    " 'SOIL_TEMPERATURE',\n",
    " 'AIR_TEMPERATURE',\n",
    " 'SOIL_HEAT_FLUX',\n",
    " 'RAINFALL_14DAYS_SUM',\n",
    " 'RAINFALL_7DAYS_SUM',\n",
    " 'WIND_SPEED',\n",
    " 'DOY',\n",
    " 'Hour_cos',\n",
    " 'NDVI']\n",
    "\n",
    "# Compute correlation matrix\n",
    "data = DF[preds_after_corr + [\"NET_CARBON_DIOXIDE_FLUX\"]]\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# Compute p-values\n",
    "p_values = pd.DataFrame(np.zeros_like(corr_matrix), columns=corr_matrix.columns, index=corr_matrix.index)\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):  # Only compute for the lower triangle\n",
    "        r, p = pearsonr(data.iloc[:, i], data.iloc[:, j])\n",
    "        p_values.iloc[i, j] = p\n",
    "        p_values.iloc[j, i] = p  # Mirror the values\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Define a function to format annotations with significance levels\n",
    "def format_annot(r, p):\n",
    "    if p < 0.01:\n",
    "        return f\"{r:.2f} **\"  # Highly significant\n",
    "    elif p < 0.05:\n",
    "        return f\"{r:.2f} *\"   # Significant\n",
    "    else:\n",
    "        return f\"{r:.2f}\"     # Not significant\n",
    "\n",
    "annot_matrix = np.vectorize(format_annot)(corr_matrix, p_values)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    corr_matrix, mask=mask, annot=annot_matrix, fmt=\"\", cmap=\"coolwarm\", linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.8}, annot_kws={\"fontsize\": 8}\n",
    ")\n",
    "plt.title(\"Correlation Matrix with Significance Levels (*p<0.05, **p<0.01), MI > 0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from scipy.stats import pearsonr\n",
    "\n",
    "# preds_after_corr = [\n",
    "#     'INCOMING_SHORTWAVE_RADIATION',\n",
    "#     'OUTGOING_LONGWAVE_RADIATION',\n",
    "#     'VAPOUR_PRESSURE_DEFICIT',\n",
    "#     'SOIL_MOISTURE',\n",
    "#     'SOIL_TEMPERATURE',\n",
    "#     'AIR_TEMPERATURE',\n",
    "#     'SOIL_HEAT_FLUX',\n",
    "#     'RAINFALL_14DAYS_SUM',\n",
    "#     'WIND_SPEED',\n",
    "#     'DOY',\n",
    "#     'Hour_cos',\n",
    "#     'NDVI']\n",
    "\n",
    "# # Compute correlation matrix\n",
    "# data = DF[preds_after_corr + [\"NET_CARBON_DIOXIDE_FLUX\"]]\n",
    "# corr_matrix = data.corr()\n",
    "\n",
    "# # corr_matrix = corr_matrix[1:][:-1]\n",
    "\n",
    "# # Compute p-values\n",
    "# p_values = pd.DataFrame(np.zeros_like(corr_matrix), columns=corr_matrix.columns, index=corr_matrix.index)\n",
    "# for i in range(len(corr_matrix.columns)):\n",
    "#     for j in range(i):  # lower triangle only\n",
    "#         r, p = pearsonr(data.iloc[:, i], data.iloc[:, j])\n",
    "#         p_values.iloc[i, j] = p\n",
    "#         p_values.iloc[j, i] = p\n",
    "\n",
    "# # Format annotations\n",
    "# def format_annot(r, p):\n",
    "#     if p < 0.01:\n",
    "#         return f\"{r:.2f} **\"\n",
    "#     elif p < 0.05:\n",
    "#         return f\"{r:.2f} *\"\n",
    "#     else:\n",
    "#         return f\"{r:.2f}\"\n",
    "# annot_matrix = np.vectorize(format_annot)(corr_matrix, p_values)\n",
    "\n",
    "# # Mask upper triangle and diagonal\n",
    "# mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=0)\n",
    "\n",
    "# # Plot heatmap\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# ax = sns.heatmap(\n",
    "#     corr_matrix, mask=mask, annot=annot_matrix, fmt=\"\", cmap=\"coolwarm\", linewidths=0.5,\n",
    "#     cbar_kws={\"shrink\": 0.8}, annot_kws={\"fontsize\": 10}\n",
    "# )\n",
    "\n",
    "# # Get tick labels\n",
    "# tick_labels = corr_matrix.columns.tolist()\n",
    "# tick_labels_x = tick_labels\n",
    "# tick_labels_y = tick_labels\n",
    "\n",
    "# tick_labels_x[-1] = ''\n",
    "# tick_labels_y[0] = ''\n",
    "# tick_labels_y[-1] = '1'\n",
    "# # # Remove top-left and bottom-right tick marks\n",
    "# # tick_labels[0] = \"\"  # top-left\n",
    "# # tick_labels[-1] = \"\"  # bottom-right\n",
    "\n",
    "# # Apply updated tick labels\n",
    "# ax.set_xticklabels(tick_labels_x, rotation=45, ha='right')\n",
    "# ax.set_yticklabels(tick_labels_y, rotation=0)\n",
    "\n",
    "# plt.title(\"Correlation Matrix with Significance Levels (*p<0.05, **p<0.01), MI > 0.1\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_labels_y[-1] = ''\n",
    "tick_labels_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[1:][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAN(Kolmogorov Arnold Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan import *\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# create a KAN: 2D inputs, 1D output, and 5 hidden neurons. cubic spline (k=3), 5 grid intervals (grid=5).\n",
    "model = KAN(width=[8,2,1], grid=5, k=6, seed=42, device=device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test\n",
    "dataset = {}\n",
    "dataset['train_input'] = torch.tensor(X_train_scaled).cuda()\n",
    "dataset['train_label'] = torch.tensor((Y_train.values).reshape(-1,1)).cuda()\n",
    "dataset['test_input'] = torch.tensor(X_test_scaled).cuda()\n",
    "dataset['test_label'] = torch.tensor((Y_test.values).reshape(-1,1)).cuda()\n",
    "dataset['train_input'].shape,dataset['train_label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot KAN at initialization\n",
    "model(dataset['train_input']);\n",
    "model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.fit(dataset, opt=\"LBFGS\", steps=150, lamb=1e-5);\n",
    "\n",
    "model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.prune()\n",
    "model.plot()\n",
    "\n",
    "model.fit(dataset, opt=\"LBFGS\", steps=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"auto\" # \"manual\"\n",
    "\n",
    "if mode == \"manual\":\n",
    "    # manual mode\n",
    "    model.fix_symbolic(0,0,0,'sin');\n",
    "    model.fix_symbolic(0,1,0,'x^2');\n",
    "    model.fix_symbolic(1,0,0,'exp');\n",
    "elif mode == \"auto\":\n",
    "    # automatic mode\n",
    "    lib = ['x','x^2','x^3','x^4','x^5','exp','log','sqrt','tanh','sin','abs']\n",
    "    model.auto_symbolic(lib=lib)\n",
    "\n",
    "\n",
    "model.fit(dataset, opt=\"LBFGS\", steps=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset, opt=\"LBFGS\", steps=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_expression(x):\n",
    "    # Validate the input vector\n",
    "    if len(x) != 8:\n",
    "        raise ValueError(\"Input vector must have exactly 8 elements.\")\n",
    "    \n",
    "    x1, x2, x3, x4, x5, x6, x7, x8 = x  # Unpack the vector elements\n",
    "\n",
    "    # Inner components of the equation\n",
    "    term_exp1 = np.exp(2.9408 * x6)\n",
    "    term_exp2 = np.exp(0.2078 * x1 + 0.3293 * x2 - 0.3442 * x3 + 0.1202 * x4 - 0.7108 * x5 - 0.2872 * x6 + 0.3426 * x7 - 0.5009 * x8)\n",
    "    term_log1 = np.log(4.0984 * x4 + 4.0509)\n",
    "    term_sin1 = np.sin(0.797 * x4 + 4.7879)\n",
    "    term_sin2 = np.sin(2.0506 * x8 - 1.1653)\n",
    "    term_sin3 = np.sin(-0.1352 * x1 + 0.0387 * x2 + 0.1899 * x3 - 0.1506 * x5 + 0.1797 * x7 + 0.1277 * x8 - 0.1877 * term_log1 + 0.0391 * np.sin(1.8887 * x6 + 10.0297) + 4.1106)\n",
    "    term_exp3 = np.exp(-1.4562 * x5)\n",
    "    term_exp4 = np.exp(-2.37 * x1)\n",
    "\n",
    "    # Inner equation for the squared term\n",
    "    inner_squared = (\n",
    "        -0.123 * x1 + 0.0537 * x2 - 0.0526 * x3 + 0.0352 * x4 + 0.1472 * x5\n",
    "        - 0.0566 * x6 - 0.0296 * x7 - 0.0089 * x8\n",
    "        + 0.0033 * term_exp1 - 0.0385 * term_exp2\n",
    "        + 0.1982 * term_sin1 + 0.2523 * term_sin2 - 0.5067 * term_sin3\n",
    "        - 1 + 0.0578 * term_exp3 + 0.1117 * term_exp4\n",
    "    ) ** 2\n",
    "\n",
    "    # Complete the equation\n",
    "    result = (\n",
    "        -0.0603 * x1 + 0.7849 * x2 - 0.1576 * x3 - 0.5702 * x4 + 0.032 * x5\n",
    "        + 0.507 * x6 + 0.5869 * x7 + 0.3115 * x8\n",
    "        - 11.1613 * inner_squared\n",
    "        - 0.0019 * term_exp1 - 0.3796 * term_log1 - 0.2401 * term_sin1\n",
    "        + 0.0791 * np.sin(1.8887 * x6 + 10.0297)\n",
    "        - 0.1288 * term_sin2 - 0.0098 - 0.0701 * term_exp3\n",
    "        + 0.7289 * term_exp4\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "vector = np.array([1, 2, 3, 4, 5, 6, 7, 8])/10\n",
    "print(compute_expression(vector))\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def compute_expression_v3(x):\n",
    "    # Validate the input vector\n",
    "    if len(x) != 8:\n",
    "        raise ValueError(\"Input vector must have exactly 8 elements.\")\n",
    "\n",
    "    x1, x2, x3, x4, x5, x6, x7, x8 = x  # Unpack the vector elements\n",
    "\n",
    "    # Inner components of the equation\n",
    "    term_exp1 = np.exp(2.4 * x6)\n",
    "    term_exp2 = np.exp(-0.3533 * x1)\n",
    "    term_exp3 = np.exp(-1.0995 * x1)\n",
    "    term_sin1 = np.sin(0.8686 * x1 - 0.0125)\n",
    "    term_sin2 = np.sin(2.2712 * x8 - 0.3693)\n",
    "    term_sin3 = np.sin(\n",
    "        -0.002 * x2 + 0.062 * x3 + 0.1167 * x4 + 0.1238 * x5 + 0.0061 * x7\n",
    "        - 0.1261 * np.sin(1.9481 * x6 + 2.1895)\n",
    "        + 0.3755 * np.sin(2.2436 * x8 + 5.7863)\n",
    "        + 7.9362 - 3.5393 * term_exp2\n",
    "    )\n",
    "\n",
    "    # Complete the equation\n",
    "    result = (\n",
    "        -12.1473 * x1 + 0.94 * x2 + 0.6179 * x3 - 0.2283 * x4 + 1.2258 * x5\n",
    "        + 0.588 * x6 + 0.629 * x7 + 0.2228 * x8\n",
    "        + 0.0117 * term_exp1 - 9.4412 * term_sin1 + 3.0657 * term_sin2\n",
    "        + 10.1083 * term_sin3 + 16.8554 - 11.7143 * term_exp3\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "# vector_v3 = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "compute_expression_v3(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle - 0.0603 x_{1} + 0.7849 x_{2} - 0.1576 x_{3} - 0.5702 x_{4} + 0.032 x_{5} + 0.507 x_{6} + 0.5869 x_{7} + 0.3115 x_{8} - 11.1613 \\left(- 0.123 x_{1} + 0.0537 x_{2} - 0.0526 x_{3} + 0.0352 x_{4} + 0.1472 x_{5} - 0.0566 x_{6} - 0.0296 x_{7} - 0.0089 x_{8} + 0.0033 e^{2.9408 x_{6}} - 0.0385 e^{0.2078 x_{1} + 0.3293 x_{2} - 0.3442 x_{3} + 0.1202 x_{4} - 0.7108 x_{5} - 0.2872 x_{6} + 0.3426 x_{7} - 0.5009 x_{8}} +      0.1982 \\sin{\\left(0.797 x_{4} + 4.7879 \\right)} + 0.2523 \\sin{\\left(2.0506 x_{8} - 1.1653 \\right)} - 0.5067 \\sin{\\left(- 0.1352 x_{1} + 0.0387 x_{2} + 0.1899 x_{3} - 0.1506 x_{5} + 0.1797 x_{7} + 0.1277 x_{8} - 0.1877 \\log{\\left(4.0984 x_{4} + 4.0509 \\right)} + 0.0391 \\sin{\\left(1.8887 x_{6} + 10.0297 \\right)} + 4.1106 \\right)} - 1 + 0.0578 e^{- 1.4562 x_{5}} + 0.1117 e^{- 2.37 x_{1}}\\right)^{2} - 0.0019 e^{2.9408 x_{6}} - 0.3796 \\log{\\left(4.0984 x_{4} + 4.0509 \\right)} - 0.2401 \\sin{\\left(0.797 x_{4} + 4.7879 \\right)} + 0.0791 \\sin{\\left(1.8887 x_{6} + 10.0297 \\right)} - 0.1288 \\sin{\\left(2.0506 x_{8} - 1.1653 \\right)} - 0.0098 - 0.0701 e^{- 1.4562 x_{5}} + 0.7289 e^{- 2.37 x_{1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "Y_train_pred = np.zeros(len(X_train))\n",
    "for i in range(len(Y_train_pred)):\n",
    "    Y_train_pred[i] = compute_expression(X_train_scaled[i])\n",
    "rmse_train = (mean_squared_error(Y_train,Y_train_pred))**0.5\n",
    "plt.scatter(Y_train,Y_train_pred)\n",
    "plt.title(f'train R2: {r2_score(Y_train,Y_train_pred)}, rmse: {rmse_train}')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Y_test_pred = np.zeros(len(X_test))\n",
    "for i in range(len(Y_test_pred)):\n",
    "    Y_test_pred[i] = compute_expression(X_test_scaled[i])\n",
    "rmse_test = (mean_squared_error(Y_test,Y_test_pred))**0.5\n",
    "plt.scatter(Y_test,Y_test_pred)\n",
    "plt.title(f'test R2: {r2_score(Y_test,Y_test_pred)}, rmse: {rmse_test}')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle - 12.1473 x_{1} + 0.94 x_{2} + 0.6179 x_{3} - 0.2283 x_{4} + 1.2258 x_{5} + 0.588 x_{6} + 0.629 x_{7} + 0.2228 x_{8} + 0.0117 e^{2.4 x_{6}} - 9.4412 \\sin{\\left(0.8686 x_{1} - 0.0125 \\right)} + 3.0657 \\sin{\\left(2.2712 x_{8} - 0.3693 \\right)} + 10.1083 \\sin{\\left(- 0.002 x_{2} + 0.062 x_{3} + 0.1167 x_{4} + 0.1238 x_{5} + 0.0061 x_{7} - 0.1261 \\sin{\\left(1.9481 x_{6} + 2.1895 \\right)} + 0.3755 \\sin{\\left(2.2436 x_{8} + 5.7863 \\right)} + 7.9362 - 3.5393 e^{- 0.3533 x_{1}} \\right)} + 16.8554 - 11.7143 e^{- 1.0995 x_{1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "Y_train_pred = np.zeros(len(X_train))\n",
    "for i in range(len(Y_train_pred)):\n",
    "    Y_train_pred[i] = compute_expression_v3(X_train_scaled[i])\n",
    "rmse_train = (mean_squared_error(Y_train,Y_train_pred))**0.5\n",
    "plt.scatter(Y_train,Y_train_pred)\n",
    "plt.title(f'train R2: {r2_score(Y_train,Y_train_pred)}, rmse: {rmse_train}')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Y_test_pred = np.zeros(len(X_test))\n",
    "for i in range(len(Y_test_pred)):\n",
    "    Y_test_pred[i] = compute_expression_v3(X_test_scaled[i])\n",
    "rmse_test = (mean_squared_error(Y_test,Y_test_pred))**0.5\n",
    "plt.scatter(Y_test,Y_test_pred)\n",
    "plt.title(f'test R2: {r2_score(Y_test,Y_test_pred)}, rmse: {rmse_test}')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.symbolic_formula()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan.utils import ex_round\n",
    "\n",
    "ex_round(model.symbolic_formula()[0][0],4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySR and GPLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysr import PySRRegressor\n",
    "\n",
    "model = PySRRegressor(\n",
    "    procs = 8,\n",
    "    populations=16, # 2x the number of processors\n",
    "    niterations=40,  # < Increase me for better results\n",
    "    binary_operators=[\"+\", \"*\",\"/\"],\n",
    "    unary_operators=[\n",
    "        \"cos\",\n",
    "        \"exp\",\n",
    "        \"sin\",\n",
    "        # \"inv(x) = 1/x\",\n",
    "        \"square\",\n",
    "        \"cube\",\n",
    "        # inv Custom operator (julia syntax)\n",
    "        \"pow\",\n",
    "    ],\n",
    "    maxsize=20,\n",
    "    maxdepth=6,\n",
    "    # extra_sympy_mappings={\"inv\": lambda x: 1 / x},\n",
    "    # ^ Define operator for SymPy as well\n",
    "    # elementwise_loss=\"loss(prediction, target) = (prediction - target)^2\",\n",
    "    # ^ Custom loss function (julia syntax)\n",
    "    nested_constraints={\n",
    "        \"square\": {\"square\": 2, \"cube\": 1, \"exp\": 0}, # Limit nesting\n",
    "        \"exp\": {\"square\": 1, \"cube\": 1, \"exp\": 0, \"cos\":0, \"sin\":0},\n",
    "    },\n",
    "\n",
    "    progress = True,\n",
    "    # complexity_of_variables = 2,\n",
    "    # constraints={\n",
    "    #     # \"pow\": (-1, 1), # Base can have arbitrary complexity while the exponent has <= 1 complexity so, x1^x2 is not possible \n",
    "    #     \"/\": (-1, 9),\n",
    "    #     \"square\": 4,\n",
    "    #     \"cube\": 4,\n",
    "    #     \"exp\": 4,\n",
    "    # },\n",
    "\n",
    "    complexity_of_constants=2, # constants punished more than variables\n",
    "\n",
    "    parsimony = 0.0008, # 0.0032 default, how much to punish complexity\n",
    "\n",
    "    weight_mutate_operator = 0.50, # Relative likelihood for mutation to swap an operator.\n",
    "    # Relative likelihood for mutation to swap an operator.\n",
    "\n",
    "    warm_start = True, # Tells fit to continue from where the last call to fit finished. \n",
    "    #If false, each call to fit will be fresh, overwriting previous results.\n",
    "\n",
    "    model_selection = 'best', # default = 'best'/'score' # Score is defined as the negated derivative of the log-loss with respect to complexity - \n",
    "    # if an expression has a much better loss at a slightly higher complexity, it is preferred\n",
    "\n",
    "    weight_randomize=0.0008, # Relative likelihood for mutation to completely delete and then randomly generate the equation\n",
    "    # Default: 0.00023\n",
    "\n",
    "    denoise = 'False', # Whether to use a Gaussian Process to denoise the data before inputting to PySR. Can help PySR fit noisy data.\n",
    "\n",
    "    # batching = 'True',\n",
    "\n",
    "    # batch_size = 100, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_points = 5000\n",
    "model.fit(X_train_scaled[:train_on_points,:], Y_train[:train_on_points])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "Y_pred = model.predict(X_test_scaled)\n",
    "plt.scatter(Y_pred, Y_test)\n",
    "plt.title(f'PySR R2 = {r2_score(Y_pred, Y_test)}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysr import PySRRegressor\n",
    "\n",
    "model_ = PySRRegressor.from_file('hall_of_fame_2024-08-02_225106.414.pkl')\n",
    "\n",
    "model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_.equations_.iloc[24]['lambda_format']\n",
    "model_.set_params(extra_sympy_mappings={'inv': lambda x: 1/x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_.equations_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_.equations_.iloc[24]['equation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "Y_pred = model_.predict(X_test_scaled,index=24)\n",
    "plt.scatter(Y_pred, Y_test)\n",
    "plt.title(f'PySR R2 = {r2_score(Y_pred, Y_test)}')\n",
    "plt.axline((0,0),(1,1),c='r')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gplearn.genetic import SymbolicRegressor\n",
    "from sympy import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from gplearn.functions import make_function\n",
    "\n",
    "\n",
    "def pow_3(x1):\n",
    "    f = x1**3\n",
    "    return f\n",
    "pow_3 = make_function(function=pow_3,name='pow3',arity=1)\n",
    "\n",
    "def pow_2(x1):\n",
    "    f = x1**2\n",
    "    return f\n",
    "pow_2 = make_function(function=pow_2,name='pow2',arity=1)\n",
    "\n",
    "function_set = ['add', 'sub', 'mul', 'div','cos','sin','neg','inv',pow_3,pow_2]\n",
    "\n",
    "# function_set = ['add', 'sub', 'mul', 'div','cos','sin','neg','inv','sqrt']\n",
    "est_gp = SymbolicRegressor(population_size=2500,function_set=function_set,\n",
    "                           generations=800, stopping_criteria=0.01,\n",
    "                           p_crossover=0.7, p_subtree_mutation=0.1,\n",
    "                           p_hoist_mutation=0.05, p_point_mutation=0.1,\n",
    "                           max_samples=0.9, verbose=1,\n",
    "                           parsimony_coefficient=0.004, random_state=0,\n",
    "                          feature_names=X_train.columns, n_jobs=12)\n",
    "\n",
    "converter = {\n",
    "    'sub': lambda x, y : x - y,\n",
    "    'div': lambda x, y : x/y,\n",
    "    'mul': lambda x, y : x*y,\n",
    "    'add': lambda x, y : x + y,\n",
    "    'neg': lambda x    : -x,\n",
    "    'pow': lambda x, y : x**y,\n",
    "    'sin': lambda x    : sin(x),\n",
    "    'cos': lambda x    : cos(x),\n",
    "    'inv': lambda x: 1/x,\n",
    "    'sqrt': lambda x: x**0.5,\n",
    "    'pow3': lambda x: x**3,\n",
    "    'exp_' : lambda x: exp(x),\n",
    "    'pow2': lambda x: x**2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_gp.fit(X_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "Y_pred = est_gp.predict(X_test_scaled)\n",
    "plt.scatter(Y_pred, Y_test)\n",
    "plt.title(f'GPLearn R2 = {r2_score(Y_pred, Y_test)}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "SEED = 42\n",
    "gap_list = [2,4,7,14,21,30]  # You can add more gaps if needed\n",
    "\n",
    "# Load NDVI\n",
    "ndvi_hw = xr.load_dataset('NDVI_HW_interpolated_30min_2022_23.netcdf', engine='netcdf4')\n",
    "ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "# Assuming DF_HW and preds_after_corr are defined outside this script\n",
    "# Example placeholders (remove or replace with your actual data):\n",
    "# DF_HW = pd.read_csv('your_data.csv')  # Load your dataframe\n",
    "# preds_after_corr = ['feature1', 'feature2', ..., 'featureN']  # Replace with actual feature names\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    # Scaling features\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    # Define time ranges\n",
    "    mid_timestamp = pd.to_datetime('2023-06-05 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    # Split data\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "    # --- Polynomial Regression ---\n",
    "    degree = 4  # You can change to 3 or higher, but be careful with overfitting\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "\n",
    "\n",
    "    model = Ridge(alpha=2.0)  # L2 regularization strength; tune as needed\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    y_pred = model.predict(X_test_poly)\n",
    "\n",
    "    # model = LinearRegression()\n",
    "    # model.fit(X_train_poly, y_train)\n",
    "    # y_pred = model.predict(X_test_poly)\n",
    "\n",
    "    # --- Metrics ---\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "    delta = (test_data['MIDPOINT_DATETIME'] - test_data['TIMESTAMP']).values[0]\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_test, color='red', linestyle='dashed',\n",
    "             label='Actual (Gap)', marker='o')\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_pred, color='green', linestyle='dashed',\n",
    "             label='Predicted (Gap-filled)', marker='o')\n",
    "    plt.axvspan(gap_start + delta, gap_end + delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(f\"HW | Polynomial Regression (deg={degree}) | {gap_days}-Day Gap | \"\n",
    "              f\"R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "preds_after_corr = ['INCOMING_SHORTWAVE_RADIATION',\n",
    " 'OUTGOING_LONGWAVE_RADIATION',\n",
    " 'VAPOUR_PRESSURE_DEFICIT',\n",
    " 'SOIL_MOISTURE',\n",
    " 'SOIL_TEMPERATURE',\n",
    " 'SOIL_HEAT_FLUX',\n",
    " 'RAINFALL_14DAYS_SUM',\n",
    " 'WIND_SPEED',\n",
    " 'DOY',\n",
    " 'Hour_cos',\n",
    " 'NDVI']\n",
    "\n",
    "ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf',engine=\"netcdf4\")\n",
    "ndvi_mean = ndvi_hw['NDVI'].mean(dim = ['x','y'])\n",
    "\n",
    "DF = DF_HW.copy(deep=True)\n",
    "DF['NDVI'] = ndvi_mean\n",
    "\n",
    "\n",
    "pred_MAT = DF[DF['NET_CARBON_DIOXIDE_FLUX_FLAG']==0][preds_after_corr] #Only considering the non-imputed flux\n",
    "target_var = DF[DF['NET_CARBON_DIOXIDE_FLUX_FLAG']==0]['NEE (g C/m2)']\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(pred_MAT, target_var,train_size=0.8,random_state=42)\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train_scaled = sc.transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_friedman2\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel,RBF\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.utils import resample\n",
    "# X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n",
    "\n",
    "# Subsample\n",
    "\n",
    "X_sub, Y_sub = resample(X_train_scaled, Y_train, n_samples=2000, random_state=0)\n",
    "\n",
    "kernel = RBF() + DotProduct() + WhiteKernel(noise_level=1)\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "gpr.fit(X_sub, Y_sub)\n",
    "\n",
    "\n",
    "\n",
    "Y_pred = gpr.predict(X_test_scaled)\n",
    "\n",
    "plt.scatter(Y_pred, Y_test)\n",
    "plt.title(r2_score(Y_pred, Y_test))\n",
    "plt.show()\n",
    "# gpr = GaussianProcessRegressor(random_state=0).fit(X_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, DotProduct\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Chronological Split (Time Series Aware)\n",
    "# ---------------------------\n",
    "split_index = int(len(pred_MAT) * 0.8)\n",
    "X_train_full = pred_MAT.iloc[:split_index]\n",
    "Y_train_full = target_var.iloc[:split_index]\n",
    "\n",
    "X_test = pred_MAT.iloc[split_index:]\n",
    "Y_test = target_var.iloc[split_index:]\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Scale Using Only Training Data\n",
    "# ---------------------------\n",
    "sc = StandardScaler()\n",
    "X_train_full_scaled = sc.fit_transform(X_train_full)\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Subsample to 2000 Points\n",
    "# ---------------------------\n",
    "X_sub, Y_sub = resample(X_train_full_scaled, Y_train_full, n_samples=2000, random_state=0)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Train GPR\n",
    "# ---------------------------\n",
    "kernel = RBF() + DotProduct() + WhiteKernel(noise_level=0.1)\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "gpr.fit(X_sub, Y_sub)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Predict with Uncertainty\n",
    "# ---------------------------\n",
    "y_pred, y_std = gpr.predict(X_test_scaled, return_std=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Plot Predictions + Uncertainty\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(Y_test.reset_index(drop=True), label='True', alpha=0.6)\n",
    "plt.plot(y_pred, label='Prediction', color='red')\n",
    "plt.fill_between(range(len(y_pred)), y_pred - 2 * y_std, y_pred + 2 * y_std,\n",
    "                 color='red', alpha=0.2, label='95% Confidence Interval')\n",
    "plt.title(\"GPR Prediction with Uncertainty (2000 training points)\")\n",
    "plt.xlabel(\"Time Index (after 80% split)\")\n",
    "plt.ylabel(\"NEE (g C/m²)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Learns GPP and RECO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUBLAS_WORKSPACE_CONFIG=:16:8  # For removing randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "\n",
    "try:\n",
    "    print(best_params)\n",
    "except:\n",
    "    best_params = {'hidden_dim': 102, 'num_layers': 1, 'learning_rate': 0.00040454706668126267}\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)  # Ensures deterministic behavior\n",
    "\n",
    "\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Sample DataFrame (replace with actual dataset)\n",
    "DF = DF_FNE.copy(deep=True)\n",
    "DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "# Define features and target\n",
    "features = ['INCOMING_SHORTWAVE_RADIATION', 'SOIL_TEMPERATURE', 'AIR_PRESSURE', \n",
    "            'VAPOUR_PRESSURE_DEFICIT', 'WIND_SPEED', 'SOIL_MOISTURE', 'SOIL_HEAT_FLUX', 'DOY']\n",
    "target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "\n",
    "# Compute Day_flag: 1 for daytime (SW >= 10 W/m²), 0 for nighttime\n",
    "DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "# Define Neural Network with tunable hyperparameters\n",
    "class NEEModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):      \n",
    "        super(NEEModel, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_dim, 16))\n",
    "        layers.append(nn.ReLU())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.E0 = nn.Linear(16, 1)\n",
    "        self.alpha = nn.Linear(16, 1)\n",
    "        self.R_ref = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x, T, PAR, Day_flag):\n",
    "        x = self.model(x)\n",
    "        E0 = self.E0(x)\n",
    "        alpha = self.alpha(x)\n",
    "        R_ref = self.R_ref(x)\n",
    "        \n",
    "        Reco = R_ref * torch.exp(E0 * ((1 / (283.15 - 261)) - (1 / (T + 273.15 - 261))))\n",
    "        GPP = alpha * (-25.9 * PAR) / (alpha * PAR - 25.9)\n",
    "        NEE = Reco - (Day_flag * GPP)\n",
    "        \n",
    "        return NEE\n",
    "\n",
    "# Function to compute metrics\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    mape = mean_absolute_percentage_error(true_values, predicted_values)\n",
    "    return rmse, r2, mape\n",
    "\n",
    "# Objective function for Optuna hyperparameter optimization\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 8, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=2)\n",
    "    \n",
    "    train_data = DF[(DF['TIMESTAMP'] < gap_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "    \n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    test_data = test_data[test_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    \n",
    "    X_train = torch.tensor(train_data[features].values, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(train_data[target].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    T_train = torch.tensor(train_data['SOIL_TEMPERATURE'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    SW_train = torch.tensor(train_data['INCOMING_SHORTWAVE_RADIATION'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    PAR_train = SW_train * 2.02\n",
    "    Day_flag_train = torch.tensor(train_data['Day_flag'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "    model = NEEModel(input_dim=X_train.shape[1], \n",
    "                 hidden_dim=best_params['hidden_dim'], \n",
    "                 num_layers=best_params['num_layers']).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    num_epochs = 500\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train, T_train, PAR_train, Day_flag_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    X_test = torch.tensor(test_data[features].values, dtype=torch.float32).to(device)\n",
    "    T_test = torch.tensor(test_data['SOIL_TEMPERATURE'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    SW_test = torch.tensor(test_data['INCOMING_SHORTWAVE_RADIATION'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    PAR_test = SW_test * 2.02\n",
    "    Day_flag_test = torch.tensor(test_data['Day_flag'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    \n",
    "    test_data['Predicted_Final'] = model(X_test, T_test, PAR_test, Day_flag_test).cpu().detach().numpy()\n",
    "    \n",
    "    actual_values = test_data[target]\n",
    "    predicted_values = test_data['Predicted_Final']\n",
    "    rmse, r2, _ = calculate_metrics(actual_values, predicted_values)\n",
    "    return r2\n",
    "\n",
    "# Run Bayesian optimization\n",
    "sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "# Run model with optimized hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(DF, gap_days, hidden_dim, num_layers, learning_rate):\n",
    "    mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    \n",
    "    train_data = DF[(DF['TIMESTAMP'] < gap_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "    \n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    test_data = test_data[test_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    \n",
    "    X_train = torch.tensor(train_data[features].values, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(train_data[target].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    T_train = torch.tensor(train_data['SOIL_TEMPERATURE'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    SW_train = torch.tensor(train_data['INCOMING_SHORTWAVE_RADIATION'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    PAR_train = SW_train * 2.02\n",
    "    Day_flag_train = torch.tensor(train_data['Day_flag'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "    model = NEEModel(input_dim=X_train.shape[1], hidden_dim=hidden_dim, num_layers=num_layers).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    num_epochs = 500\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train, T_train, PAR_train, Day_flag_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    X_test = torch.tensor(test_data[features].values, dtype=torch.float32).to(device)\n",
    "    T_test = torch.tensor(test_data['SOIL_TEMPERATURE'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    SW_test = torch.tensor(test_data['INCOMING_SHORTWAVE_RADIATION'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    PAR_test = SW_test * 2.02\n",
    "    Day_flag_test = torch.tensor(test_data['Day_flag'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    \n",
    "    test_data['Predicted_Final'] = model(X_test, T_test, PAR_test, Day_flag_test).cpu().detach().numpy()\n",
    "    \n",
    "    actual_values = test_data[target]\n",
    "    predicted_values = test_data['Predicted_Final']\n",
    "    \n",
    "    rmse, r2, mape = calculate_metrics(actual_values, predicted_values)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(test_data['TIMESTAMP'], actual_values, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    plt.plot(test_data['TIMESTAMP'], predicted_values, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    plt.axvspan(gap_start, gap_end, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "    plt.title(f\"FNE | NN with Day_flag | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "best_params = study.best_params\n",
    "train_and_evaluate(DF, gap_days=7, \n",
    "                   hidden_dim=best_params['hidden_dim'], \n",
    "                   num_layers=best_params['num_layers'],\n",
    "                   learning_rate=best_params['learning_rate'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)  # Ensures deterministic behavior\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(DF, gap_days, hidden_dim, num_layers, learning_rate):\n",
    "    mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    \n",
    "    train_data = DF[(DF['TIMESTAMP'] < gap_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "    \n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    test_data = test_data[test_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    \n",
    "    X_train = torch.tensor(train_data[features].values, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(train_data[target].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    T_train = torch.tensor(train_data['SOIL_TEMPERATURE'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    SW_train = torch.tensor(train_data['INCOMING_SHORTWAVE_RADIATION'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    PAR_train = SW_train * 2.02\n",
    "    Day_flag_train = torch.tensor(train_data['Day_flag'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "    model = NEEModel(input_dim=X_train.shape[1], hidden_dim=hidden_dim, num_layers=num_layers).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    num_epochs = 500\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train, T_train, PAR_train, Day_flag_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    X_test = torch.tensor(test_data[features].values, dtype=torch.float32).to(device)\n",
    "    T_test = torch.tensor(test_data['SOIL_TEMPERATURE'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    SW_test = torch.tensor(test_data['INCOMING_SHORTWAVE_RADIATION'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    PAR_test = SW_test * 2.02\n",
    "    Day_flag_test = torch.tensor(test_data['Day_flag'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    \n",
    "    test_data['Predicted_Final'] = model(X_test, T_test, PAR_test, Day_flag_test).cpu().detach().numpy()\n",
    "    \n",
    "    actual_values = test_data[target]\n",
    "    predicted_values = test_data['Predicted_Final']\n",
    "    \n",
    "    rmse, r2, mape = calculate_metrics(actual_values, predicted_values)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(test_data['TIMESTAMP'], actual_values, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    plt.plot(test_data['TIMESTAMP'], predicted_values, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    plt.axvspan(gap_start, gap_end, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "    plt.title(f\"FNE | NN with Day_flag | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "best_params = study.best_params\n",
    "train_and_evaluate(DF, gap_days=7, \n",
    "                   hidden_dim=best_params['hidden_dim'], \n",
    "                   num_layers=best_params['num_layers'],\n",
    "                   learning_rate=best_params['learning_rate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Sample DataFrame (replace with actual dataset)\n",
    "DF = DF_FNE.copy(deep=True)\n",
    "DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "# Define features and target\n",
    "features = ['INCOMING_SHORTWAVE_RADIATION', 'SOIL_TEMPERATURE', 'AIR_PRESSURE', \n",
    "            'VAPOUR_PRESSURE_DEFICIT', 'WIND_SPEED', 'SOIL_MOISTURE', 'SOIL_HEAT_FLUX', 'DOY']\n",
    "target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "\n",
    "# Compute Day_flag: 1 for daytime (SW >= 10 W/m²), 0 for nighttime\n",
    "DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "class NEEModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):      \n",
    "        super(NEEModel, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_dim, 16))\n",
    "        layers.append(nn.ReLU())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.E0 = nn.Linear(16, 1)\n",
    "        self.alpha = nn.Linear(16, 1)\n",
    "        self.R_ref = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x, T, PAR, Day_flag):\n",
    "        x = self.model(x)\n",
    "        E0 = self.E0(x)\n",
    "        alpha = self.alpha(x)\n",
    "        R_ref = self.R_ref(x)\n",
    "        \n",
    "        Reco = R_ref * torch.exp(E0 * ((1 / (283.15 - 261)) - (1 / (T + 273.15 - 261))))\n",
    "        GPP = alpha * (-25.9 * PAR) / (alpha * PAR - 25.9)\n",
    "        NEE = Reco - (Day_flag * GPP)\n",
    "        \n",
    "        return NEE\n",
    "\n",
    "# Function to compute metrics\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    mape = mean_absolute_percentage_error(true_values, predicted_values)\n",
    "    return rmse, r2, mape\n",
    "\n",
    "# Function to train and evaluate model for different gap sizes\n",
    "def plot_gap(DF, gap_days, ax):\n",
    "    mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    \n",
    "    train_data = DF[(DF['TIMESTAMP'] < gap_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "    \n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    test_data = test_data[test_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    \n",
    "    X_train = torch.tensor(train_data[features].values, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(train_data[target].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    T_train = torch.tensor(train_data['SOIL_TEMPERATURE'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    SW_train = torch.tensor(train_data['INCOMING_SHORTWAVE_RADIATION'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    PAR_train = SW_train * 2.02\n",
    "    Day_flag_train = torch.tensor(train_data['Day_flag'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "    model = NEEModel(input_dim=X_train.shape[1], \n",
    "                     hidden_dim=best_params[\"hidden_dim\"],\n",
    "                     num_layers=best_params[\"num_layers\"]).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "    \n",
    "    num_epochs = 1000\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train, T_train, PAR_train, Day_flag_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    X_test = torch.tensor(test_data[features].values, dtype=torch.float32).to(device)\n",
    "    T_test = torch.tensor(test_data['SOIL_TEMPERATURE'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    SW_test = torch.tensor(test_data['INCOMING_SHORTWAVE_RADIATION'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    PAR_test = SW_test * 2.02\n",
    "    Day_flag_test = torch.tensor(test_data['Day_flag'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    \n",
    "    test_data['Predicted_Final'] = model(X_test, T_test, PAR_test, Day_flag_test).cpu().detach().numpy()\n",
    "    \n",
    "    actual_values = test_data[target]\n",
    "    predicted_values = test_data['Predicted_Final']\n",
    "    \n",
    "    rmse, r2, mape = calculate_metrics(actual_values, predicted_values)\n",
    "    \n",
    "    ax.plot(test_data['TIMESTAMP'], actual_values, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    ax.plot(test_data['TIMESTAMP'], predicted_values, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    ax.axvspan(gap_start, gap_end, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "    ax.set_title(f\"FNE | NN with Day_flag | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    ax.set_xlabel(\"Timestamp\")\n",
    "    ax.set_ylabel(\"CO₂ Flux\")\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "# Plot results for multiple gap sizes\n",
    "gap_sizes = [2, 4, 7, 14, 21, 30]\n",
    "fig, axes = plt.subplots(len(gap_sizes), 1, figsize=(12, 3 * len(gap_sizes)))\n",
    "for i, gap in enumerate(gap_sizes):\n",
    "    plot_gap(DF, gap, axes[i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'hidden_dim': 102, 'num_layers': 1, 'learning_rate': 0.00040454706668126267}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUBLAS_WORKSPACE_CONFIG=:16:8  # For removing randomness\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "\n",
    "  # Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)  # Ensures deterministic behavior\n",
    "\n",
    "# gap_days = 30\n",
    "NN_hyperparameters = {}\n",
    "for gap_days in [2,4,7,14,21,30]:\n",
    "    try:\n",
    "        print(best_params)\n",
    "    except:\n",
    "        best_params = {'hidden_dim': 53, 'num_layers': 4, 'learning_rate': 0.0029106359131330704}\n",
    "\n",
    "\n",
    "    # Check for GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Sample DataFrame (replace with actual dataset)\n",
    "    DF = DF_FNE.copy(deep=True)\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    # Define features and target\n",
    "    features = ['INCOMING_SHORTWAVE_RADIATION', 'SOIL_TEMPERATURE', 'AIR_PRESSURE', \n",
    "                'VAPOUR_PRESSURE_DEFICIT', 'WIND_SPEED', 'SOIL_MOISTURE', 'SOIL_HEAT_FLUX', 'DOY']\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "\n",
    "    # Compute Day_flag: 1 for daytime (SW >= 10 W/m²), 0 for nighttime\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    # Define Neural Network with tunable hyperparameters\n",
    "    class NEEModel(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, num_layers):      \n",
    "            super(NEEModel, self).__init__()\n",
    "            layers = []\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            for _ in range(num_layers - 1):\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(hidden_dim, 16))\n",
    "            layers.append(nn.ReLU())\n",
    "            self.model = nn.Sequential(*layers)\n",
    "            self.E0 = nn.Linear(16, 1)\n",
    "            self.alpha = nn.Linear(16, 1)\n",
    "            self.R_ref = nn.Linear(16, 1)\n",
    "\n",
    "        def forward(self, x, T, PAR, Day_flag):\n",
    "            x = self.model(x)\n",
    "            E0 = self.E0(x)\n",
    "            alpha = self.alpha(x)\n",
    "            R_ref = self.R_ref(x)\n",
    "            \n",
    "            Reco = R_ref * torch.exp(E0 * ((1 / (283.15 - 261)) - (1 / (T + 273.15 - 261))))\n",
    "            GPP = alpha * (-25.9 * PAR) / (alpha * PAR - 25.9)\n",
    "            NEE = Reco - (Day_flag * GPP)\n",
    "            \n",
    "            return NEE\n",
    "\n",
    "    # Function to compute metrics\n",
    "    def calculate_metrics(true_values, predicted_values):\n",
    "        rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "        r2 = r2_score(true_values, predicted_values)\n",
    "        mape = mean_absolute_percentage_error(true_values, predicted_values)\n",
    "        return rmse, r2, mape\n",
    "\n",
    "    # Objective function for Optuna hyperparameter optimization\n",
    "    def objective(trial):\n",
    "        hidden_dim = trial.suggest_int(\"hidden_dim\", 8, 128)\n",
    "        num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "        mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "        gap_start = mid_timestamp - pd.Timedelta(days=gap_days//2)\n",
    "        gap_end = mid_timestamp + pd.Timedelta(days=gap_days//2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Define Validation Region (same duration as test, just before it)\n",
    "        val_end = gap_start  # Validation ends where test starts\n",
    "        val_start = val_end - (gap_end - gap_start)  # Same duration as test\n",
    "\n",
    "        # Update Training Region (exclude validation & test)\n",
    "        train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "\n",
    "        # Define Validation & Test Data\n",
    "        val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "        test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "        \n",
    "\n",
    "        train_data = DF[(DF['TIMESTAMP'] < gap_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "        test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "        \n",
    "        train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "        test_data = test_data[test_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "\n",
    "        val_data = val_data[val_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "        \n",
    "        X_train = torch.tensor(train_data[features].values, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(train_data[target].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        T_train = torch.tensor(train_data['SOIL_TEMPERATURE'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        SW_train = torch.tensor(train_data['INCOMING_SHORTWAVE_RADIATION'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        PAR_train = SW_train * 2.02\n",
    "        Day_flag_train = torch.tensor(train_data['Day_flag'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "        model = NEEModel(input_dim=X_train.shape[1], \n",
    "                    hidden_dim=best_params['hidden_dim'], \n",
    "                    num_layers=best_params['num_layers']).to(device)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        num_epochs = 500\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train, T_train, PAR_train, Day_flag_train)\n",
    "            loss = criterion(outputs, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        X_val = torch.tensor(val_data[features].values, dtype=torch.float32).to(device)\n",
    "        T_val = torch.tensor(val_data['SOIL_TEMPERATURE'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        SW_val = torch.tensor(val_data['INCOMING_SHORTWAVE_RADIATION'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        PAR_val = SW_val * 2.02\n",
    "        Day_flag_val = torch.tensor(val_data['Day_flag'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        \n",
    "        val_data['Predicted_Final'] = model(X_val, T_val, PAR_val, Day_flag_val).cpu().detach().numpy()\n",
    "        \n",
    "        actual_values = val_data[target]\n",
    "        predicted_values = val_data['Predicted_Final']\n",
    "        rmse, r2, _ = calculate_metrics(actual_values, predicted_values)\n",
    "        return rmse\n",
    "\n",
    "    # Run Bayesian optimization\n",
    "    sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=200)\n",
    "\n",
    "    # Print best hyperparameters\n",
    "    print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "    # Run model with optimized hyperparameters\n",
    "    best_params = study.best_params\n",
    "\n",
    "\n",
    "\n",
    "    #### NOTE : FOR HYPERPARAMETER OPTIMIZATION A VALIDATION SET IS CREATED JUST BESIDE THE TEST SET\n",
    "\n",
    "\n",
    "    def train_and_evaluate(DF, gap_days, hidden_dim, num_layers, learning_rate):\n",
    "        mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "        gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "        gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "        \n",
    "        train_data = DF[(DF['TIMESTAMP'] < gap_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "        test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "        \n",
    "        train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "        test_data = test_data[test_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "        \n",
    "        X_train = torch.tensor(train_data[features].values, dtype=torch.float32).to(device)\n",
    "        y_train = torch.tensor(train_data[target].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        T_train = torch.tensor(train_data['SOIL_TEMPERATURE'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        SW_train = torch.tensor(train_data['INCOMING_SHORTWAVE_RADIATION'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        PAR_train = SW_train * 2.02\n",
    "        Day_flag_train = torch.tensor(train_data['Day_flag'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "        model = NEEModel(input_dim=X_train.shape[1], hidden_dim=hidden_dim, num_layers=num_layers).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        num_epochs = 500\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train, T_train, PAR_train, Day_flag_train)\n",
    "            loss = criterion(outputs, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        X_test = torch.tensor(test_data[features].values, dtype=torch.float32).to(device)\n",
    "        T_test = torch.tensor(test_data['SOIL_TEMPERATURE'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        SW_test = torch.tensor(test_data['INCOMING_SHORTWAVE_RADIATION'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        PAR_test = SW_test * 2.02\n",
    "        Day_flag_test = torch.tensor(test_data['Day_flag'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        \n",
    "        test_data['Predicted_Final'] = model(X_test, T_test, PAR_test, Day_flag_test).cpu().detach().numpy()\n",
    "        \n",
    "        actual_values = test_data[target]\n",
    "        predicted_values = test_data['Predicted_Final']\n",
    "        \n",
    "        rmse, r2, mape = calculate_metrics(actual_values, predicted_values)\n",
    "        \n",
    "        # Plot results\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(test_data['TIMESTAMP'], actual_values, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "        plt.plot(test_data['TIMESTAMP'], predicted_values, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "        plt.axvspan(gap_start, gap_end, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "        plt.title(f\"FNE | NN with Prior | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "        plt.xlabel(\"Timestamp\")\n",
    "        plt.ylabel(\"CO₂ Flux\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    best_params = study.best_params\n",
    "    NN_hyperparameters[f'{gap_days}'] = best_params\n",
    "    train_and_evaluate(DF, gap_days=gap_days, \n",
    "                    hidden_dim=best_params['hidden_dim'], \n",
    "                    num_layers=best_params['num_layers'],\n",
    "                    learning_rate=best_params['learning_rate'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score, root_mean_squared_error\n",
    "import xarray as xr\n",
    "\n",
    "# Set random seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "gap_list = [2, 4, 7, 14, 21, 30]\n",
    "\n",
    "# Store best params\n",
    "CNN_hyperparameters = {}\n",
    "\n",
    "# CNN Model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_length, hidden_channels, kernel_size):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, hidden_channels, kernel_size=kernel_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(hidden_channels * (input_length - kernel_size + 1), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    # Load NDVI\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].mean(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    # Define features and target\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "\n",
    "    # Day/Night flag\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    # Time split\n",
    "    mid_timestamp = pd.to_datetime('2023-06-05 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    val_data = val_data[val_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "\n",
    "    # Prepare tensors\n",
    "    def to_tensor(df):\n",
    "        X = torch.tensor(df[features].values, dtype=torch.float32).unsqueeze(1)  # (batch, channels=1, length=features)\n",
    "        y = torch.tensor(df[target].values, dtype=torch.float32)\n",
    "        return X, y\n",
    "\n",
    "    X_train, y_train = to_tensor(train_data)\n",
    "    X_val, y_val = to_tensor(val_data)\n",
    "    X_test, y_test = to_tensor(test_data)\n",
    "\n",
    "    # Optuna Objective\n",
    "    def objective(trial):\n",
    "        hidden_channels = trial.suggest_int('hidden_channels', 8, 64)\n",
    "        kernel_size = trial.suggest_int('kernel_size', 2, min(5, len(features)))\n",
    "        lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "\n",
    "        model = SimpleCNN(input_length=len(features), hidden_channels=hidden_channels, kernel_size=kernel_size).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        n_epochs = 100\n",
    "        batch_size = 256\n",
    "\n",
    "        # Training Loop\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            permutation = torch.randperm(X_train.size(0))\n",
    "            for i in range(0, X_train.size(0), batch_size):\n",
    "                indices = permutation[i:i+batch_size]\n",
    "                batch_x, batch_y = X_train[indices].to(device), y_train[indices].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(X_val.to(device)).cpu().numpy()\n",
    "            true = y_val.numpy()\n",
    "            rmse = np.sqrt(mean_squared_error(true, preds))\n",
    "\n",
    "        return rmse\n",
    "\n",
    "    # Run Optuna\n",
    "    sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "    study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(f\"Best hyperparameters for {gap_days}-day gap:\", best_params)\n",
    "\n",
    "    CNN_hyperparameters[f\"{gap_days}\"] = best_params\n",
    "\n",
    "    # Train best model and plot\n",
    "    hidden_channels = best_params['hidden_channels']\n",
    "    kernel_size = best_params['kernel_size']\n",
    "    lr = best_params['lr']\n",
    "    weight_decay = best_params['weight_decay']\n",
    "\n",
    "    model = SimpleCNN(input_length=len(features), hidden_channels=hidden_channels, kernel_size=kernel_size).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Final training\n",
    "    n_epochs = 200\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        permutation = torch.randperm(X_train.size(0))\n",
    "        for i in range(0, X_train.size(0), 256):\n",
    "            indices = permutation[i:i+256]\n",
    "            batch_x, batch_y = X_train[indices].to(device), y_train[indices].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_test.to(device)).cpu().numpy()\n",
    "        true = y_test.numpy()\n",
    "\n",
    "    r2 = r2_score(true, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(true, preds))\n",
    "    mape = mean_absolute_percentage_error(true, preds)\n",
    "\n",
    "    delta = (test_data['MIDPOINT_DATETIME'] - test_data['TIMESTAMP']).values[0]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], true, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], preds, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    plt.axvspan(gap_start+delta, gap_end+delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "    plt.title(f\"HW | 1D CNN | {gap_days}-Day Gap | R²: {round(r2,2)}, RMSE: {round(rmse,2)}, MAPE: {round(mape,2)}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates  \n",
    "\n",
    "SEED = 42\n",
    "gap_list = [2, 4, 7, 14, 21, 30]\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    # Load NDVI\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    \n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-05 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "    # --- Train and predict with SVM ---\n",
    "    model = SVR(C=5, epsilon=0.1, kernel='rbf')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # --- Metrics ---\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "    delta = (test_data['MIDPOINT_DATETIME'] - test_data['TIMESTAMP']).values[0]\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_pred, color='blue', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    plt.axvspan(gap_start + delta, gap_end + delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(f\"HW | SVM (SVR) | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "def rbf(x, center, gamma):\n",
    "    return np.exp(-gamma * np.linalg.norm(x - center, axis=1) ** 2)\n",
    "\n",
    "SEED = 42\n",
    "gap_list = [2, 4, 7, 14, 21, 30]\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    # Load NDVI\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    \n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-05 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    X_train, y_train = train_data[features].values, train_data[target].values\n",
    "    X_test, y_test = test_data[features].values, test_data[target].values\n",
    "\n",
    "    # --- RBF Network ---\n",
    "    n_centers = 5000  # Number of RBF units\n",
    "    gamma = 0.5      # RBF width parameter\n",
    "\n",
    "    # 1. Find centers using KMeans\n",
    "    kmeans = KMeans(n_clusters=n_centers, random_state=SEED)\n",
    "    kmeans.fit(X_train)\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    # 2. Compute RBF features\n",
    "    X_train_rbf = np.array([rbf(X_train, c, gamma) for c in centers]).T\n",
    "    X_test_rbf = np.array([rbf(X_test, c, gamma) for c in centers]).T\n",
    "\n",
    "    # 3. Fit a linear model on top\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train_rbf, y_train)\n",
    "    y_pred = model.predict(X_test_rbf)\n",
    "\n",
    "    # --- Metrics ---\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "    delta = (test_data['MIDPOINT_DATETIME'] - test_data['TIMESTAMP']).values[0]\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_pred, color='purple', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    plt.axvspan(gap_start + delta, gap_end + delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(f\"HW | RBF Network | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression + NEE equation as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates  \n",
    "\n",
    "SEED = 42\n",
    "gap_list = [2, 4, 7, 14, 21, 30]\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    # Load NDVI\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    # Define features and compute Predicted_Symbolic\n",
    "    features = preds_after_corr\n",
    "\n",
    "    R_ref = 1.24\n",
    "    E_0 = 54\n",
    "    T_ref = 283.15\n",
    "    T_0 = 261\n",
    "    GPP_opt = -25.9\n",
    "    alpha = 0.0081\n",
    "\n",
    "    def compute_fluxes(row):\n",
    "        T = row['SOIL_TEMPERATURE'] + 273.15\n",
    "        SW = row['INCOMING_SHORTWAVE_RADIATION']\n",
    "        PAR = SW * 2.02\n",
    "        Reco = R_ref * np.exp(E_0 * ((1 / (T_ref - T_0)) - (1 / (T - T_0))))\n",
    "        GPP = alpha * (GPP_opt * PAR) / (alpha * PAR + GPP_opt)\n",
    "        NEE = Reco - GPP\n",
    "        return NEE\n",
    "\n",
    "    DF['Predicted_Symbolic'] = DF.apply(compute_fluxes, axis=1)\n",
    "\n",
    "    # Update features list\n",
    "    features = preds_after_corr + ['Predicted_Symbolic']\n",
    "\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    # Standardize all features including Predicted_Symbolic\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-05 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    X_train, y_train = train_data[features], train_data['NET_CARBON_DIOXIDE_FLUX']\n",
    "    X_test, y_test = test_data[features], test_data['NET_CARBON_DIOXIDE_FLUX']\n",
    "\n",
    "    # --- Train Ridge Regression instead of XGBoost ---\n",
    "    model = Ridge(alpha=0.01, random_state=SEED)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # --- Metrics ---\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "    delta = (test_data['MIDPOINT_DATETIME'] - test_data['TIMESTAMP']).values[0]\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_pred, color='blue', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    plt.axvspan(gap_start + delta, gap_end + delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(f\"HW | Ridge Regression | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_after_corr = ['INCOMING_SHORTWAVE_RADIATION',\n",
    " 'OUTGOING_LONGWAVE_RADIATION',\n",
    " 'VAPOUR_PRESSURE_DEFICIT',\n",
    " 'SOIL_MOISTURE',\n",
    " 'SOIL_TEMPERATURE',\n",
    " 'SOIL_HEAT_FLUX',\n",
    " 'RAINFALL_14DAYS_SUM',\n",
    " 'WIND_SPEED',\n",
    " 'DOY',\n",
    " 'Hour_cos',\n",
    " 'NDVI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score, root_mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "RF_hyperparameters = {}\n",
    "gap_list = [2]\n",
    "\n",
    "best_params_list_rf = {}\n",
    "# gap_list = [2]\n",
    "for gap_days in gap_list:\n",
    "    # Sample DataFrame (replace with actual dataset)\n",
    "    ndvi_hw = xr.load_dataset('NDVI_HW_interpolated_30min_2022_23.netcdf',engine=\"netcdf4\")\n",
    "    ndvi_mean = ndvi_hw['NDVI'].mean(dim = ['x','y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    \n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "\n",
    "    # Define features and target\n",
    "    # features = ['INCOMING_SHORTWAVE_RADIATION', 'SOIL_TEMPERATURE', 'AIR_PRESSURE', \n",
    "    #             'VAPOUR_PRESSURE_DEFICIT', 'WIND_SPEED', 'SOIL_MOISTURE', 'SOIL_HEAT_FLUX', 'DOY']\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "\n",
    "    # Compute Day_flag: 1 for daytime (SW >= 10 W/m²), 0 for nighttime\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    # Define Validation and Test Split\n",
    "\n",
    "    # mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "    mid_timestamp = pd.to_datetime('2023-06-05 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    # Define Validation Region (same duration as test, just before it)\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    # Remove flagged values\n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    val_data = val_data[val_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    # test_data = test_data[test_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "\n",
    "    # Extract features and target\n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_val, y_val = val_data[features], val_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "    # Hyperparameter Optimization using Optuna\n",
    "    def objective(trial):\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 5)\n",
    "        max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "        \n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators, \n",
    "            max_depth=max_depth, \n",
    "            min_samples_split=min_samples_split, \n",
    "            min_samples_leaf=min_samples_leaf, \n",
    "            max_features=max_features,\n",
    "            random_state=SEED, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        rmse = root_mean_squared_error(y_val,y_pred)\n",
    "        return rmse  # minimize\n",
    "\n",
    "    # Run Optuna optimization\n",
    "    sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=200)\n",
    "\n",
    "    # Print best hyperparameters\n",
    "    best_params = study.best_params\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "    RF_hyperparameters[f'{gap_days}'] = best_params\n",
    "    # Train and Evaluate with Optimized Random Forest\n",
    "    def train_and_evaluate(X_train, y_train, X_test, y_test, best_params):\n",
    "        model = RandomForestRegressor(\n",
    "            **best_params, random_state=SEED, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "        delta = (test_data['MIDPOINT_DATETIME']-test_data['TIMESTAMP']).values[0]\n",
    "        # Plot results\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(test_data['MIDPOINT_DATETIME'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "        plt.plot(test_data['MIDPOINT_DATETIME'], y_pred, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "        plt.axvspan(gap_start+delta, gap_end+delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "        plt.title(f\"HW | Random Forest | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "        plt.xlabel(\"Timestamp\")\n",
    "        plt.ylabel(\"CO₂ Flux\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    train_and_evaluate(X_train, y_train, X_test, y_test, best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_hyperparameters['14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(RF_hyperparameters,'RF_hyperparameters.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running for loaded hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "RF_hyperparameters = joblib.load('RF_hyperparameters.pkl')\n",
    "\n",
    "\n",
    "\n",
    "preds_after_corr = ['INCOMING_SHORTWAVE_RADIATION',\n",
    " 'OUTGOING_LONGWAVE_RADIATION',\n",
    " 'VAPOUR_PRESSURE_DEFICIT',\n",
    " 'SOIL_MOISTURE',\n",
    " 'SOIL_TEMPERATURE',\n",
    " 'SOIL_HEAT_FLUX',\n",
    " 'RAINFALL_14DAYS_SUM',\n",
    " 'WIND_SPEED',\n",
    " 'DOY',\n",
    " 'Hour_cos',\n",
    " 'NDVI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates  \n",
    "\n",
    "SEED = 42\n",
    "gap_list = [2,4,7,14,21,30]\n",
    "\n",
    "# Make sure RF_hyperparameters is preloaded\n",
    "# Example:\n",
    "# RF_hyperparameters = {\n",
    "#     '2': {'n_estimators': 150, 'max_depth': 20, ...},\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    # Load NDVI\n",
    "    ndvi_hw = xr.load_dataset('NDVI_HW_interpolated_30min_2022_23.netcdf',engine='netcdf4')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    # DF = DF[DF['MIDPOINT_DATETIME'].dt.year == 2023]\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    \n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-05 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    # print(len)\n",
    "    # train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    # val_data = val_data[val_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "\n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "    # Train and predict using saved best hyperparameters\n",
    "    best_params = RF_hyperparameters[str(gap_days)]\n",
    "    model = RandomForestRegressor(**best_params, random_state=SEED, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "    delta = (test_data['MIDPOINT_DATETIME']-test_data['TIMESTAMP']).values[0]\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_pred, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    plt.axvspan(gap_start + delta, gap_end + delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(f\"HW | Random Forest | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates  \n",
    "\n",
    "SEED = 42\n",
    "gap_list = [4, 7, 14]\n",
    "\n",
    "fig, axs = plt.subplots(len(gap_list), 1, figsize=(12, 15))\n",
    "\n",
    "for i, gap_days in enumerate(gap_list):\n",
    "    # Load NDVI\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-15 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "    best_params = RF_hyperparameters[str(gap_days)]\n",
    "    model = RandomForestRegressor(**best_params, random_state=SEED, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "    delta = (test_data['MIDPOINT_DATETIME'] - test_data['TIMESTAMP']).values[0]\n",
    "\n",
    "    ax = axs[i]\n",
    "    ax.plot(test_data['MIDPOINT_DATETIME'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    ax.plot(test_data['MIDPOINT_DATETIME'], y_pred, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    ax.axvspan(gap_start + delta, gap_end + delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "\n",
    "    ax.set_ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    ax.set_title(f\"HW | Random Forest | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    ax.grid()\n",
    "    if i == len(gap_list) - 1:\n",
    "        ax.set_xlabel(\"Timestamp\")\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d\\n%H:%M'))\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates  \n",
    "\n",
    "SEED = 42\n",
    "gap_list = [4, 7, 14]\n",
    "\n",
    "fig, axs = plt.subplots(len(gap_list), 1, figsize=(12, 15))\n",
    "\n",
    "for i, gap_days in enumerate(gap_list):\n",
    "    # Load NDVI\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-05 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "    # --- First model ---\n",
    "    best_params = RF_hyperparameters[str(gap_days)]\n",
    "    base_model = RandomForestRegressor(**best_params, random_state=SEED, n_jobs=-1)\n",
    "    base_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_train = base_model.predict(X_train)\n",
    "    y_pred_test = base_model.predict(X_test)\n",
    "\n",
    "    # --- Second model: bias corrector ---\n",
    "    residual_train = y_train - y_pred_train\n",
    "    bias_model = RandomForestRegressor(random_state=SEED, n_jobs=-1)\n",
    "    bias_model.fit(X_train, residual_train)\n",
    "\n",
    "    bias_correction = bias_model.predict(X_test)\n",
    "    y_pred_corrected = y_pred_test + bias_correction\n",
    "\n",
    "    # --- Metrics ---\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_corrected))\n",
    "    r2 = r2_score(y_test, y_pred_corrected)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred_corrected)\n",
    "\n",
    "    # --- Plotting ---\n",
    "    delta = (test_data['MIDPOINT_DATETIME'] - test_data['TIMESTAMP']).values[0]\n",
    "    ax = axs[i]\n",
    "    ax.plot(test_data['MIDPOINT_DATETIME'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    ax.plot(test_data['MIDPOINT_DATETIME'], y_pred_corrected, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    ax.axvspan(gap_start + delta, gap_end + delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "\n",
    "    ax.set_ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    ax.set_title(f\"HW | Stacked RF Correction | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    ax.grid()\n",
    "\n",
    "    if i == len(gap_list) - 1:\n",
    "        ax.set_xlabel(\"Timestamp\")\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d\\n%H:%M'))\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "gap_list = [2, 4, 7, 14, 21, 30]\n",
    "n_bootstraps = 20\n",
    "sample_size = 17520//2\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    \n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-05 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    # Filter valid flags\n",
    "    # train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    # test_data = test_data[test_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "    test_dates = test_data['MIDPOINT_DATETIME']\n",
    "    delta = (test_data['MIDPOINT_DATETIME'] - test_data['TIMESTAMP']).values[0]\n",
    "\n",
    "    preds_all = []\n",
    "    r2_all, rmse_all, mae_all,mape_all = [], [], [], []\n",
    "\n",
    "    best_params = RF_hyperparameters[str(gap_days)]\n",
    "\n",
    "    for b in tqdm(range(n_bootstraps), desc=f\"Bootstrapping {gap_days}-day gap\"):\n",
    "        sample = train_data.sample(n=sample_size, random_state=SEED + b)\n",
    "        X_train, y_train = sample[features], sample[target]\n",
    "\n",
    "        model = RandomForestRegressor(**best_params, random_state=SEED + b, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        preds_all.append(y_pred)\n",
    "        rmse_all.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        mae_all.append(np.mean(np.abs(y_test - y_pred)))\n",
    "        mape_all.append(mean_absolute_percentage_error(y_test, y_pred))\n",
    "        r2_all.append(r2_score(y_test, y_pred))\n",
    "        mae_mean, mae_std = np.mean(mae_all), np.std(mae_all)\n",
    "        mape_mean = np.mean(mape_all)\n",
    "\n",
    "    preds_all = np.array(preds_all)\n",
    "    y_pred_mean = preds_all.mean(axis=0)\n",
    "    y_pred_std = preds_all.std(axis=0)\n",
    "\n",
    "    # Compute mean ± 2σ performance metrics\n",
    "    r2_mean, r2_std = np.mean(r2_all), np.std(r2_all)\n",
    "    rmse_mean, rmse_std = np.mean(rmse_all), np.std(rmse_all)\n",
    "    mae_mean, mae_std = np.mean(mae_all), np.std(mae_all)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(test_dates, y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    plt.plot(test_dates, y_pred_mean, color='green', linestyle='dashed', label='Predicted (Mean)', marker='o')\n",
    "    plt.fill_between(test_dates, y_pred_mean - 2*y_pred_std, y_pred_mean + 2*y_pred_std, \n",
    "                     color='green', alpha=0.3, label='2σ Band')\n",
    "    plt.axvspan(gap_start + delta, gap_end + delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    plt.title(\n",
    "        f\"HW | Random Forest | {gap_days}-Day Gap\\n\"\n",
    "        f\"R² = {r2_mean:.2f} ± {2*r2_std:.2f}, \"\n",
    "        f\"RMSE = {rmse_mean:.2f} ± {2*rmse_std:.2f}, \"\n",
    "        f\"MAE = {mae_mean:.2f} ± {2*mae_std:.2f}, \"\n",
    "        f\"MAPE ≈ {mape_mean:.2f}\"\n",
    "    )\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Correct (New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "gap_list = [4, 7, 14]\n",
    "n_bootstraps = 20\n",
    "sample_size = 17520 // 2\n",
    "\n",
    "fig, axs = plt.subplots(len(gap_list), 1, figsize=(12, 15))  # Independent x-axes\n",
    "\n",
    "for i, gap_days in enumerate(gap_list):\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-15 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "    test_dates = test_data['MIDPOINT_DATETIME']\n",
    "    delta = (test_data['MIDPOINT_DATETIME'] - test_data['TIMESTAMP']).values[0]\n",
    "\n",
    "    preds_all = []\n",
    "    r2_all, rmse_all, mae_all, mape_all = [], [], [], []\n",
    "\n",
    "    best_params = RF_hyperparameters[str(gap_days)]\n",
    "\n",
    "    for b in tqdm(range(n_bootstraps), desc=f\"Bootstrapping {gap_days}-day gap\"):\n",
    "        sample = train_data.sample(n=sample_size, random_state=SEED + b)\n",
    "        X_train, y_train = sample[features], sample[target]\n",
    "\n",
    "        model = RandomForestRegressor(**best_params, random_state=SEED + b, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        preds_all.append(y_pred)\n",
    "        rmse_all.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        mae_all.append(np.mean(np.abs(y_test - y_pred)))\n",
    "        mape_all.append(mean_absolute_percentage_error(y_test, y_pred))\n",
    "        r2_all.append(r2_score(y_test, y_pred))\n",
    "\n",
    "    preds_all = np.array(preds_all)\n",
    "    y_pred_mean = preds_all.mean(axis=0)\n",
    "    y_pred_std = preds_all.std(axis=0)\n",
    "\n",
    "    r2_mean, r2_std = np.mean(r2_all), np.std(r2_all)\n",
    "    rmse_mean, rmse_std = np.mean(rmse_all), np.std(rmse_all)\n",
    "    mae_mean, mae_std = np.mean(mae_all), np.std(mae_all)\n",
    "    mape_mean = np.mean(mape_all)\n",
    "\n",
    "    ax = axs[i]\n",
    "    ax.plot(test_dates, y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    ax.plot(test_dates, y_pred_mean, color='green', linestyle='dashed', label='Predicted (Mean)', marker='o')\n",
    "    ax.fill_between(test_dates, y_pred_mean - 2 * y_pred_std, y_pred_mean + 2 * y_pred_std,\n",
    "                    color='green', alpha=0.3, label='2σ Band')\n",
    "    ax.axvspan(gap_start + delta, gap_end + delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "\n",
    "    ax.set_ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    ax.set_title(\n",
    "        f\"HW | Random Forest | {gap_days}-Day Gap\\n\"\n",
    "        f\"R² = {r2_mean:.2f} ± {2*r2_std:.2f}, \"\n",
    "        f\"RMSE = {rmse_mean:.2f} ± {2*rmse_std:.2f}, \"\n",
    "        f\"MAE = {mae_mean:.2f} ± {2*mae_std:.2f}, \"\n",
    "        f\"MAPE ≈ {mape_mean:.2f}\"\n",
    "    )\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d\\n%H:%M'))\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid()\n",
    "    ax.legend()  # show legend on every subplot\n",
    "\n",
    "    if i == len(gap_list) - 1:\n",
    "        ax.set_xlabel(\"Timestamp\")\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "    if i == len(gap_list) - 1:\n",
    "        ax.set_xlabel(\"Timestamp\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_HW[DF_HW['MIDPOINT_DATETIME'].dt.year == 2023]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF with NEE equation as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates  \n",
    "\n",
    "SEED = 42\n",
    "gap_list = [2, 4, 7, 14, 21, 30]\n",
    "\n",
    "# Make sure RF_hyperparameters is preloaded\n",
    "# Example:\n",
    "# RF_hyperparameters = {\n",
    "#     '2': {'n_estimators': 150, 'max_depth': 20, ...},\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "for gap_days in gap_list:\n",
    "# Load NDVI\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    # Define features and add Predicted_Symbolic\n",
    "    features = preds_after_corr\n",
    "\n",
    "    R_ref = 1.24\n",
    "    E_0 = 54\n",
    "    T_ref = 283.15\n",
    "    T_0 = 261\n",
    "    GPP_opt = -25.9\n",
    "    alpha = 0.0081\n",
    "\n",
    "    def compute_fluxes(row):\n",
    "        T = row['SOIL_TEMPERATURE'] + 273.15\n",
    "        SW = row['INCOMING_SHORTWAVE_RADIATION']\n",
    "        PAR = SW * 2.02\n",
    "        Reco = R_ref * np.exp(E_0 * ((1 / (T_ref - T_0)) - (1 / (T - T_0))))\n",
    "        GPP = alpha * (GPP_opt * PAR) / (alpha * PAR + GPP_opt)\n",
    "        NEE = Reco - GPP\n",
    "        return NEE\n",
    "\n",
    "    DF['Predicted_Symbolic'] = DF.apply(compute_fluxes, axis=1)\n",
    "\n",
    "    # Update features to include symbolic prediction\n",
    "    features = preds_after_corr + ['Predicted_Symbolic']\n",
    "\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    # Standardize including Predicted_Symbolic\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-05 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    # print(len)\n",
    "    # train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    # val_data = val_data[val_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "\n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "    # Train and predict using saved best hyperparameters\n",
    "    best_params = RF_hyperparameters[str(gap_days)]\n",
    "    model = RandomForestRegressor(**best_params, random_state=SEED, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "    delta = (test_data['MIDPOINT_DATETIME']-test_data['TIMESTAMP']).values[0]\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_pred, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    plt.axvspan(gap_start + delta, gap_end + delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(f\"HW | Random Forest | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "gap_list = [2, 4, 7, 14, 21, 30]\n",
    "mid_timestamps = pd.date_range('2022-09-01', '2023-06-30', freq='15D')\n",
    "\n",
    "# Load NDVI (do it only once)\n",
    "ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    print(f\"\\nProcessing gap: {gap_days} days\")\n",
    "\n",
    "    metrics_list = []\n",
    "\n",
    "    for mid_timestamp in tqdm(mid_timestamps, desc=f'Gap {gap_days}'):\n",
    "        DF = DF_HW.copy(deep=True)\n",
    "        DF['NDVI'] = ndvi_mean\n",
    "        DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "        features = preds_after_corr\n",
    "        target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "        \n",
    "        DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "        gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "        gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "        val_end = gap_start\n",
    "        val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "        train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "        val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "        test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "        train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "        val_data = val_data[val_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "\n",
    "        X_train, y_train = train_data[features], train_data[target]\n",
    "        X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "        if len(X_test) == 0 or len(X_train) == 0:\n",
    "            continue\n",
    "\n",
    "        # Train model\n",
    "        best_params = RF_hyperparameters[str(gap_days)]\n",
    "        model = RandomForestRegressor(**best_params, random_state=SEED, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Compute metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "        metrics_list.append({\n",
    "            \"GAP_DAYS\": gap_days,\n",
    "            \"MID_TIMESTAMP\": mid_timestamp,\n",
    "            \"RMSE\": rmse,\n",
    "            \"R2\": r2,\n",
    "            \"MAPE\": mape\n",
    "        })\n",
    "\n",
    "    all_metrics.extend(metrics_list)\n",
    "\n",
    "# Convert to DataFrame\n",
    "metrics_df = pd.DataFrame(all_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot R2 scores for different gaps\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=metrics_df, x=\"GAP_DAYS\", y=\"R2\")\n",
    "plt.title(\"Distribution of R² across different gaps\")\n",
    "plt.ylabel(\"R² Score\")\n",
    "plt.xlabel(\"Gap Days\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# RMSE\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=metrics_df, x=\"GAP_DAYS\", y=\"RMSE\")\n",
    "plt.title(\"Distribution of RMSE across different gaps\")\n",
    "plt.ylabel(\"RMSE (µmol m⁻² s⁻¹)\")\n",
    "plt.xlabel(\"Gap Days\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# MAPE\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=metrics_df, x=\"GAP_DAYS\", y=\"MAPE\")\n",
    "plt.title(\"Distribution of MAPE across different gaps\")\n",
    "plt.ylabel(\"MAPE\")\n",
    "plt.xlabel(\"Gap Days\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_after_corr = ['INCOMING_SHORTWAVE_RADIATION',\n",
    "'OUTGOING_LONGWAVE_RADIATION',\n",
    " 'VAPOUR_PRESSURE_DEFICIT',\n",
    " 'SOIL_MOISTURE',\n",
    " 'SOIL_TEMPERATURE',\n",
    " 'SOIL_HEAT_FLUX',\n",
    " 'RAINFALL_14DAYS_SUM',\n",
    " 'WIND_SPEED',\n",
    " 'DOY',\n",
    " 'Hour_cos',\n",
    " 'NDVI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 42\n",
    "gap_list = [30]\n",
    "\n",
    "# Make sure RF_hyperparameters is preloaded\n",
    "# Example:\n",
    "# RF_hyperparameters = {\n",
    "#     '2': {'n_estimators': 150, 'max_depth': 20, ...},\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    # Load NDVI\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    \n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-05 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    val_data = val_data[val_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "\n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "    # Train and predict using saved best hyperparameters\n",
    "    best_params = RF_hyperparameters[str(gap_days)]\n",
    "    best_params['n_estimators'] = 200\n",
    "    best_params['max_depth'] = 50\n",
    "    model = RandomForestRegressor(**best_params, random_state=SEED, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "    delta = (test_data['MIDPOINT_DATETIME']-test_data['TIMESTAMP']).values[0]\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_pred, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    plt.axvspan(gap_start+delta, gap_end+delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "    plt.title(f\"HW | Random Forest | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_data['MIDPOINT_DATETIME']-test_data['TIMESTAMP']).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_FSW['STARTING_DATETIME'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# List of seeds to use (10 values evenly spaced from 20 to 200)\n",
    "seed_list = [20, 40, 60, 80, 100, 120, 140, 160, 180, 200]\n",
    "\n",
    "# Containers for storing final predictions and performance metrics across seeds\n",
    "all_predictions = []  # will be a list of arrays (one per seed)\n",
    "all_metrics = []      # list of dicts with keys: 'rmse', 'r2', 'mae'\n",
    "\n",
    "# For this example, we assume that DF_HW and preds_after_corr are already defined\n",
    "# gap_list: if you want to work with different gap durations; here we use gap_list = [2]\n",
    "gap_list = [2]\n",
    "for gap_days in gap_list:\n",
    "    # Prepare dataframe (copy your dataset and convert TIMESTAMP)\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "    \n",
    "    # Define features and target\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    \n",
    "    # Compute Day_flag (1 for daytime, 0 for nighttime)\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "    \n",
    "    # Define Validation and Test split based on timestamps and gap_days\n",
    "    mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    \n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "    \n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "    \n",
    "    # Remove flagged rows from train and validation sets\n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    val_data = val_data[val_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    \n",
    "    # Extract features and target for train, validation, and test sets\n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_val, y_val = val_data[features], val_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "    \n",
    "    # Temporary lists to store predictions and metrics for this gap_days run\n",
    "    predictions_seed = []\n",
    "    metrics_seed = []\n",
    "    \n",
    "    # Loop over each seed value\n",
    "    for seed in seed_list:\n",
    "        # Set the numpy seed for reproducibility in this iteration\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Define the objective for Optuna optimization\n",
    "        def objective(trial):\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "            min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "            min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 5)\n",
    "            max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "            \n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=n_estimators, \n",
    "                max_depth=max_depth, \n",
    "                min_samples_split=min_samples_split, \n",
    "                min_samples_leaf=min_samples_leaf, \n",
    "                max_features=max_features,\n",
    "                random_state=seed, \n",
    "                n_jobs=-1\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_val)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "            return rmse  # minimize RMSE\n",
    "        \n",
    "        # Run Optuna with the current seed for reproducibility\n",
    "        sampler = optuna.samplers.TPESampler(seed=seed)\n",
    "        study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "        study.optimize(objective, n_trials=200, show_progress_bar=False)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        \n",
    "        # Train the final model using the best hyperparameters\n",
    "        model = RandomForestRegressor(**best_params, random_state=seed, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Compute performance metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        \n",
    "        # Store the predictions and metrics for this seed\n",
    "        predictions_seed.append(y_pred)\n",
    "        metrics_seed.append({'rmse': rmse, 'r2': r2, 'mae': mae})\n",
    "    \n",
    "    # Convert predictions to a numpy array of shape (n_seeds, n_test_samples)\n",
    "    predictions_seed = np.array(predictions_seed)\n",
    "    \n",
    "    # Compute the mean and 1-sigma standard deviation of predictions (time series)\n",
    "    mean_prediction = predictions_seed.mean(axis=0)\n",
    "    std_prediction = predictions_seed.std(axis=0)\n",
    "    \n",
    "    # Compute overall performance metric averages and their standard deviations\n",
    "    rmse_list = [m['rmse'] for m in metrics_seed]\n",
    "    r2_list = [m['r2'] for m in metrics_seed]\n",
    "    mae_list = [m['mae'] for m in metrics_seed]\n",
    "    \n",
    "    rmse_mean, rmse_std = np.mean(rmse_list), np.std(rmse_list)\n",
    "    r2_mean, r2_std = np.mean(r2_list), np.std(r2_list)\n",
    "    mae_mean, mae_std = np.mean(mae_list), np.std(mae_list)\n",
    "    \n",
    "    # Plot the actual test values, the mean prediction, and the uncertainty band (±1σ)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(test_data['TIMESTAMP'], y_test, 'r--o', label='Actual')\n",
    "    plt.plot(test_data['TIMESTAMP'], mean_prediction, 'g--o', label='Mean Prediction')\n",
    "    plt.fill_between(test_data['TIMESTAMP'],\n",
    "                     mean_prediction - std_prediction,\n",
    "                     mean_prediction + std_prediction,\n",
    "                     color='gray', alpha=0.3, label='± 1σ')\n",
    "    plt.axvspan(gap_start, gap_end, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux\")\n",
    "    plt.title(f\"Random Forest Predictions with Uncertainty (Gap: {gap_days} days)\\n\"\n",
    "              f\"RMSE: {rmse_mean:.2f} ± {rmse_std:.2f}, \"\n",
    "              f\"R²: {r2_mean:.2f} ± {r2_std:.2f}, \"\n",
    "              f\"MAE: {mae_mean:.2f} ± {mae_std:.2f}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_data['TIMESTAMP'], y_test, 'r--o', label='Actual')\n",
    "plt.plot(test_data['TIMESTAMP'], mean_prediction, 'g--o', label='Mean Prediction')\n",
    "plt.fill_between(test_data['TIMESTAMP'],\n",
    "                    mean_prediction - std_prediction,\n",
    "                    mean_prediction + std_prediction,\n",
    "                    color='gray', alpha=0.3, label='± 1σ')\n",
    "# plt.axvspan(gap_start, gap_end, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"CO₂ Flux\")\n",
    "plt.title(f\"Random Forest Predictions with Uncertainty (Gap: {gap_days} days)\\n\"\n",
    "            f\"RMSE: {rmse_mean:.2f} ± {rmse_std:.2f}, \"\n",
    "            f\"R²: {r2_mean:.2f} ± {r2_std:.2f}, \"\n",
    "            f\"MAE: {mae_mean:.2f} ± {mae_std:.2f}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Start from{'n_estimators': 58, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_after_corr = ['INCOMING_SHORTWAVE_RADIATION',\n",
    " 'OUTGOING_LONGWAVE_RADIATION',\n",
    " 'VAPOUR_PRESSURE_DEFICIT',\n",
    " 'SOIL_MOISTURE',\n",
    " 'SOIL_TEMPERATURE',\n",
    " 'SOIL_HEAT_FLUX',\n",
    " 'RAINFALL_14DAYS_SUM',\n",
    " 'WIND_SPEED',\n",
    " 'DOY',\n",
    " 'Hour_cos',\n",
    " 'NDVI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "XGB_hyperparameters = {}\n",
    "gap_list = [2,4,7,14,21,30]\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].mean(dim = ['x','y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    \n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "    \n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    \n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "    \n",
    "    mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "    \n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "    \n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    val_data = val_data[val_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    \n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_val, y_val = val_data[features], val_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "            \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "            \"tree_method\": \"gpu_hist\",  # Use \"gpu_hist\" if a compatible GPU is available\n",
    "            \"verbosity\": 0,\n",
    "            \"early_stopping_rounds\" :40\n",
    "        }\n",
    "        model = xgb.XGBRegressor(**params, random_state=SEED, n_jobs=-1)\n",
    "        # Enable early stopping with a validation set\n",
    "        model.fit(X_train, y_train, \n",
    "                  eval_set=[(X_val, y_val)], \n",
    "                  verbose=False)\n",
    "        y_pred = model.predict(X_val)\n",
    "        return mean_squared_error(y_val, y_pred, squared=False)  # RMSE\n",
    "    \n",
    "    sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=200)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "    \n",
    "    XGB_hyperparameters[f'{gap_days}'] = best_params\n",
    "    \n",
    "    def train_and_evaluate(X_train, y_train, X_test, y_test, best_params):\n",
    "        best_params.update({\"tree_method\": \"gpu_hist\", \"verbosity\": 0})\n",
    "        model = xgb.XGBRegressor(**best_params, random_state=SEED, n_jobs=-1,early_stopping_rounds=40)\n",
    "        model.fit(X_train, y_train, \n",
    "                  eval_set=[(X_val, y_val)], \n",
    "                  verbose=False)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        \n",
    "        delta = (test_data['MIDPOINT_DATETIME']-test_data['TIMESTAMP']).values[0]\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(test_data['TIMESTAMP'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "        plt.plot(test_data['TIMESTAMP'], y_pred, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "        plt.axvspan(gap_start+delta, gap_end+delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "        plt.title(f\"HW | XGBoost | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "        plt.xlabel(\"Timestamp\")\n",
    "        plt.ylabel(\"CO₂ Flux\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    train_and_evaluate(X_train, y_train, X_test, y_test, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_hyperparameters = {'2': {'n_estimators': 439,\n",
    "  'max_depth': 8,\n",
    "  'learning_rate': 0.07722495243348557,\n",
    "  'subsample': 0.8929440047019218,\n",
    "  'colsample_bytree': 0.8333409269922045,\n",
    "  'gamma': 1.4861413126771159e-08,\n",
    "  'lambda': 4.334391766633569e-05,\n",
    "  'alpha': 0.10501889249570763,\n",
    "  'tree_method': 'gpu_hist',\n",
    "  'verbosity': 0},\n",
    " '4': {'n_estimators': 438,\n",
    "  'max_depth': 6,\n",
    "  'learning_rate': 0.08271647603673657,\n",
    "  'subsample': 0.5691580124316945,\n",
    "  'colsample_bytree': 0.9472669637848105,\n",
    "  'gamma': 0.058993677343623575,\n",
    "  'lambda': 0.0024739873202235148,\n",
    "  'alpha': 0.05459988678061846,\n",
    "  'tree_method': 'gpu_hist',\n",
    "  'verbosity': 0},\n",
    " '7': {'n_estimators': 240,\n",
    "  'max_depth': 11,\n",
    "  'learning_rate': 0.14809232243695958,\n",
    "  'subsample': 0.9676961460895083,\n",
    "  'colsample_bytree': 0.9852065000671854,\n",
    "  'gamma': 0.9409994986953132,\n",
    "  'lambda': 8.41614092221556e-06,\n",
    "  'alpha': 1.2580936947404866e-05,\n",
    "  'tree_method': 'gpu_hist',\n",
    "  'verbosity': 0},\n",
    " '14': {'n_estimators': 380,\n",
    "  'max_depth': 4,\n",
    "  'learning_rate': 0.24296426462188053,\n",
    "  'subsample': 0.7667635685568693,\n",
    "  'colsample_bytree': 0.8927529763822221,\n",
    "  'gamma': 1.4563038375894288e-08,\n",
    "  'lambda': 4.61646365415651e-08,\n",
    "  'alpha': 0.015296796133491844,\n",
    "  'tree_method': 'gpu_hist',\n",
    "  'verbosity': 0},\n",
    " '21': {'n_estimators': 251,\n",
    "  'max_depth': 3,\n",
    "  'learning_rate': 0.1254934375669986,\n",
    "  'subsample': 0.6842689123959284,\n",
    "  'colsample_bytree': 0.8886436299703148,\n",
    "  'gamma': 3.6877440777197192e-06,\n",
    "  'lambda': 0.0001984805598631067,\n",
    "  'alpha': 0.0070381820081986185,\n",
    "  'tree_method': 'gpu_hist',\n",
    "  'verbosity': 0},\n",
    " '30': {'n_estimators': 468,\n",
    "  'max_depth': 4,\n",
    "  'learning_rate': 0.10323066745047015,\n",
    "  'subsample': 0.5348703173998094,\n",
    "  'colsample_bytree': 0.9592242186666384,\n",
    "  'gamma': 5.059284205911557e-08,\n",
    "  'lambda': 0.003240884024747558,\n",
    "  'alpha': 0.0019430308813874009,\n",
    "  'tree_method': 'gpu_hist',\n",
    "  'verbosity': 0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_after_corr = ['INCOMING_SHORTWAVE_RADIATION',\n",
    " 'OUTGOING_LONGWAVE_RADIATION',\n",
    " 'VAPOUR_PRESSURE_DEFICIT',\n",
    " 'SOIL_MOISTURE',\n",
    " 'SOIL_TEMPERATURE',\n",
    " 'SOIL_HEAT_FLUX',\n",
    " 'RAINFALL_14DAYS_SUM',\n",
    " 'WIND_SPEED',\n",
    " 'DOY',\n",
    " 'Hour_cos',\n",
    " 'NDVI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF[preds_after_corr+['NET_CARBON_DIOXIDE_FLUX']].to_csv('CO2_NDVI_check.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Hyperparameters and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates  \n",
    "\n",
    "\n",
    "SEED = 42\n",
    "gap_list = [2,4, 7, 14, 21, 30]\n",
    "\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    # DF = DF[DF['MIDPOINT_DATETIME'].dt.year == 2023]\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-15 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "\n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "    best_params = XGB_hyperparameters[str(gap_days)]\n",
    "    model = xgb.XGBRegressor(**best_params, random_state=SEED, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "    delta = (test_data['MIDPOINT_DATETIME'] - test_data['TIMESTAMP']).values[0]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_pred, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    plt.axvspan(gap_start + delta, gap_end + delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(f\"HW | XGBoost | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates  \n",
    "\n",
    "SEED = 42\n",
    "gap_list = [4, 7, 14]\n",
    "\n",
    "fig, axs = plt.subplots(len(gap_list), 1, figsize=(12, 15))  # Independent x-axes\n",
    "\n",
    "for i, gap_days in enumerate(gap_list):\n",
    "    # Load NDVI\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-15 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "    best_params = XGB_hyperparameters[str(gap_days)]\n",
    "    model = xgb.XGBRegressor(**best_params, random_state=SEED, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "    delta = (test_data['MIDPOINT_DATETIME'] - test_data['TIMESTAMP']).values[0]\n",
    "\n",
    "    ax = axs[i]\n",
    "    ax.plot(test_data['MIDPOINT_DATETIME'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    ax.plot(test_data['MIDPOINT_DATETIME'], y_pred, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    ax.axvspan(gap_start + delta, gap_end + delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "\n",
    "    ax.set_ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    ax.set_title(f\"HW | XGBoost | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d\\n%H:%M'))\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid()\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "    if i == len(gap_list) - 1:\n",
    "        ax.set_xlabel(\"Timestamp\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEE equation as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates  \n",
    "\n",
    "\n",
    "SEED = 42\n",
    "gap_list = [2, 4, 7, 14, 21, 30]\n",
    "\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    # Load NDVI\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    # Define features and add Predicted_Symbolic\n",
    "    features = preds_after_corr\n",
    "\n",
    "    R_ref = 1.24\n",
    "    E_0 = 54\n",
    "    T_ref = 283.15\n",
    "    T_0 = 261\n",
    "    GPP_opt = -25.9\n",
    "    alpha = 0.0081\n",
    "\n",
    "    def compute_fluxes(row):\n",
    "        T = row['SOIL_TEMPERATURE'] + 273.15\n",
    "        SW = row['INCOMING_SHORTWAVE_RADIATION']\n",
    "        PAR = SW * 2.02\n",
    "        Reco = R_ref * np.exp(E_0 * ((1 / (T_ref - T_0)) - (1 / (T - T_0))))\n",
    "        GPP = alpha * (GPP_opt * PAR) / (alpha * PAR + GPP_opt)\n",
    "        NEE = Reco - GPP\n",
    "        return NEE\n",
    "\n",
    "    DF['Predicted_Symbolic'] = DF.apply(compute_fluxes, axis=1)\n",
    "\n",
    "    # Update features to include symbolic prediction\n",
    "    features = preds_after_corr + ['Predicted_Symbolic']\n",
    "\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    # Standardize including Predicted_Symbolic\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-05 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "\n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "    best_params = XGB_hyperparameters[str(gap_days)]\n",
    "    model = xgb.XGBRegressor(**best_params, random_state=SEED, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "    delta = (test_data['MIDPOINT_DATETIME'] - test_data['TIMESTAMP']).values[0]\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    plt.plot(test_data['MIDPOINT_DATETIME'], y_pred, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    plt.axvspan(gap_start + delta, gap_end + delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(f\"HW | XGBoost | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates  \n",
    "\n",
    "SEED = 42\n",
    "gap_list = [2, 4, 7, 14]\n",
    "\n",
    "fig, axs = plt.subplots(len(gap_list), 1, figsize=(12, 15))  # Independent x-axes\n",
    "\n",
    "for i, gap_days in enumerate(gap_list):\n",
    "    # Load NDVI\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-05 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "    best_params = XGB_hyperparameters[str(gap_days)]\n",
    "    model = xgb.XGBRegressor(**best_params, random_state=SEED, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "    delta = (test_data['MIDPOINT_DATETIME'] - test_data['TIMESTAMP']).values[0]\n",
    "\n",
    "    ax = axs[i]\n",
    "    ax.plot(test_data['MIDPOINT_DATETIME'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    ax.plot(test_data['MIDPOINT_DATETIME'], y_pred, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    ax.axvspan(gap_start + delta, gap_end + delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "\n",
    "    ax.set_ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    ax.set_title(f\"HW | XGBoost | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d\\n%H:%M'))\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid()\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "    if i == len(gap_list) - 1:\n",
    "        ax.set_xlabel(\"Timestamp\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Booststrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "gap_list = [2, 4, 7, 14, 21, 30]\n",
    "n_bootstraps = 20\n",
    "sample_size = 17520//2\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-05 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "    test_dates = test_data['MIDPOINT_DATETIME']\n",
    "    delta = (test_data['MIDPOINT_DATETIME'] - test_data['TIMESTAMP']).values[0]\n",
    "\n",
    "    preds_all = []\n",
    "    r2_all, rmse_all, mae_all = [], [], []\n",
    "    mape_all = []\n",
    "\n",
    "    best_params = XGB_hyperparameters[str(gap_days)]\n",
    "\n",
    "    for b in tqdm(range(n_bootstraps), desc=f\"Bootstrapping XGB {gap_days}-day gap\"):\n",
    "        sample = train_data.sample(n=sample_size, random_state=SEED + b)\n",
    "        X_train, y_train = sample[features], sample[target]\n",
    "\n",
    "        model = xgb.XGBRegressor(**best_params, random_state=SEED + b, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        preds_all.append(y_pred)\n",
    "        rmse_all.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        mae_all.append(mean_absolute_error(y_test, y_pred))\n",
    "        mape_all.append(mean_absolute_percentage_error(y_test, y_pred))\n",
    "        r2_all.append(r2_score(y_test, y_pred))\n",
    "\n",
    "    preds_all = np.array(preds_all)\n",
    "    y_pred_mean = preds_all.mean(axis=0)\n",
    "    y_pred_std = preds_all.std(axis=0)\n",
    "\n",
    "    r2_mean, r2_std = np.mean(r2_all), np.std(r2_all)\n",
    "    rmse_mean, rmse_std = np.mean(rmse_all), np.std(rmse_all)\n",
    "    mae_mean, mae_std = np.mean(mae_all), np.std(mae_all)\n",
    "    mape_mean = np.mean(mape_all)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(test_dates, y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    plt.plot(test_dates, y_pred_mean, color='green', linestyle='dashed', label='Predicted (Mean)', marker='o')\n",
    "    plt.fill_between(test_dates, y_pred_mean - 2*y_pred_std, y_pred_mean + 2*y_pred_std,\n",
    "                     color='green', alpha=0.3, label='2σ Confidence Band')\n",
    "    plt.axvspan(gap_start + delta, gap_end + delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    plt.title(\n",
    "        f\"HW | XGBoost | {gap_days}-Day Gap\\n\"\n",
    "        f\"R² = {r2_mean:.2f} ± {2*r2_std:.2f}, \"\n",
    "        f\"RMSE = {rmse_mean:.2f} ± {2*rmse_std:.2f}, \"\n",
    "        f\"MAE = {mae_mean:.2f} ± {2*mae_std:.2f}, \"\n",
    "        f\"MAPE ≈ {mape_mean:.2f}\"\n",
    "    )\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "gap_list = [4, 7, 14]\n",
    "n_bootstraps = 20\n",
    "sample_size = 17520 // 2\n",
    "\n",
    "fig, axs = plt.subplots(len(gap_list), 1, figsize=(12, 15))  # Independent x-axes\n",
    "\n",
    "for i, gap_days in enumerate(gap_list):\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-05 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "    test_dates = test_data['MIDPOINT_DATETIME']\n",
    "    delta = (test_data['MIDPOINT_DATETIME'] - test_data['TIMESTAMP']).values[0]\n",
    "\n",
    "    preds_all = []\n",
    "    r2_all, rmse_all, mae_all, mape_all = [], [], [], []\n",
    "\n",
    "    best_params = XGB_hyperparameters[str(gap_days)]\n",
    "\n",
    "    for b in tqdm(range(n_bootstraps), desc=f\"Bootstrapping XGB {gap_days}-day gap\"):\n",
    "        sample = train_data.sample(n=sample_size, random_state=SEED + b)\n",
    "        X_train, y_train = sample[features], sample[target]\n",
    "\n",
    "        model = xgb.XGBRegressor(**best_params, random_state=SEED + b, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        preds_all.append(y_pred)\n",
    "        rmse_all.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "        mae_all.append(mean_absolute_error(y_test, y_pred))\n",
    "        mape_all.append(mean_absolute_percentage_error(y_test, y_pred))\n",
    "        r2_all.append(r2_score(y_test, y_pred))\n",
    "\n",
    "    preds_all = np.array(preds_all)\n",
    "    y_pred_mean = preds_all.mean(axis=0)\n",
    "    y_pred_std = preds_all.std(axis=0)\n",
    "\n",
    "    r2_mean, r2_std = np.mean(r2_all), np.std(r2_all)\n",
    "    rmse_mean, rmse_std = np.mean(rmse_all), np.std(rmse_all)\n",
    "    mae_mean, mae_std = np.mean(mae_all), np.std(mae_all)\n",
    "    mape_mean = np.mean(mape_all)\n",
    "\n",
    "    ax = axs[i]\n",
    "    ax.plot(test_dates, y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    ax.plot(test_dates, y_pred_mean, color='green', linestyle='dashed', label='Predicted (Mean)', marker='o')\n",
    "    ax.fill_between(test_dates, y_pred_mean - 2 * y_pred_std, y_pred_mean + 2 * y_pred_std,\n",
    "                    color='green', alpha=0.3, label='2σ Confidence Band')\n",
    "    ax.axvspan(gap_start + delta, gap_end + delta, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "\n",
    "    ax.set_ylabel(\"CO₂ Flux (µmol m⁻² s⁻¹)\")\n",
    "    ax.set_title(\n",
    "        f\"HW | XGBoost | {gap_days}-Day Gap\\n\"\n",
    "        f\"R² = {r2_mean:.2f} ± {2*r2_std:.2f}, \"\n",
    "        f\"RMSE = {rmse_mean:.2f} ± {2*rmse_std:.2f}, \"\n",
    "        f\"MAE = {mae_mean:.2f} ± {2*mae_std:.2f}, \"\n",
    "        f\"MAPE ≈ {mape_mean:.2f}\"\n",
    "    )\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d\\n%H:%M'))\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    # ax.grid()\n",
    "    ax.grid()\n",
    "    ax.legend()  # show legend on every subplot\n",
    "\n",
    "    if i == len(gap_list) - 1:\n",
    "        ax.set_xlabel(\"Timestamp\")\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "    if i == len(gap_list) - 1:\n",
    "        ax.set_xlabel(\"Timestamp\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF vs XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# Parameters\n",
    "SEED = 42\n",
    "gap_list = [2, 4, 7, 14, 21, 30]\n",
    "n_bootstraps = 20\n",
    "sample_size = 17520 // 2\n",
    "\n",
    "# Storage\n",
    "results = []\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    print(f\"\\nProcessing {gap_days}-day gap...\")\n",
    "\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].max(dim=['x', 'y'])\n",
    "\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "    mid_timestamp = pd.to_datetime('2023-06-05 23:58:30')\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    test_data = test_data[test_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "\n",
    "    best_params_xgb = XGB_hyperparameters[str(gap_days)]\n",
    "    best_params_rf = RF_hyperparameters[str(gap_days)]\n",
    "\n",
    "    for model_name, model_class, best_params in [\n",
    "        (\"XGBoost\", xgb.XGBRegressor, best_params_xgb),\n",
    "        (\"Random Forest\", RandomForestRegressor, best_params_rf)\n",
    "    ]:\n",
    "        print(f\" → Bootstrapping {model_name}\")\n",
    "        for b in tqdm(range(n_bootstraps), desc=f\"{model_name} {gap_days}-day\"):\n",
    "            sample = train_data.sample(n=sample_size, random_state=SEED + b)\n",
    "            X_train, y_train = sample[features], sample[target]\n",
    "\n",
    "            model = model_class(**best_params, random_state=SEED + b, n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            results.append({\n",
    "                'Model': model_name,\n",
    "                'Gap': gap_days,\n",
    "                'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "                'R2': r2_score(y_test, y_pred)\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Boxplot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# R2 Boxplot\n",
    "results_df.boxplot(column='R2', by=['Gap', 'Model'], ax=axes[0])\n",
    "axes[0].set_title('R² by Gap Size and Model')\n",
    "axes[0].set_ylabel('R²')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# RMSE Boxplot\n",
    "results_df.boxplot(column='RMSE', by=['Gap', 'Model'], ax=axes[1])\n",
    "axes[1].set_title('RMSE by Gap Size and Model')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.suptitle('Bootstrapped Model Performance Comparison')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use acronyms for model\n",
    "results_df['Model'] = results_df['Model'].replace({\n",
    "    'Random Forest': 'RF',\n",
    "    'XGBoost': 'XGB'\n",
    "})\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# R² Boxplot\n",
    "sns.boxplot(data=results_df, x='Gap', y='R2', hue='Model', ax=axes[0])\n",
    "axes[0].set_title('R² by Gap Size and Model')\n",
    "axes[0].set_ylabel('R²')\n",
    "axes[0].set_xlabel('Gap (days)')\n",
    "axes[0].legend(title='Model')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# RMSE Boxplot\n",
    "sns.boxplot(data=results_df, x='Gap', y='RMSE', hue='Model', ax=axes[1])\n",
    "axes[1].set_title('RMSE by Gap Size and Model')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_xlabel('Gap (days)')\n",
    "axes[1].legend(title='Model')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.suptitle('Bootstrapped Model Performance Comparison (RF vs XGB)', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_after_corr = ['INCOMING_SHORTWAVE_RADIATION',\n",
    " 'OUTGOING_LONGWAVE_RADIATION',\n",
    " 'VAPOUR_PRESSURE_DEFICIT',\n",
    " 'SOIL_MOISTURE',\n",
    " 'SOIL_TEMPERATURE',\n",
    " 'SOIL_HEAT_FLUX',\n",
    " 'RAINFALL_14DAYS_SUM',\n",
    " 'WIND_SPEED',\n",
    " 'DOY',\n",
    " 'Hour_cos',\n",
    " 'NDVI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize JS visualization for SHAP (useful in notebooks)\n",
    "shap.initjs()\n",
    "\n",
    "for gap_days in [2]:\n",
    "    # Prepare the dataset\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    ndvi_hw = xr.load_dataset('/scratch/users/rajarshi/Quanterra/NDVI_HW_interpolated_30min_2022_23.netcdf')\n",
    "    ndvi_mean = ndvi_hw['NDVI'].median(dim = ['x','y'])\n",
    "\n",
    "    DF['NDVI'] = ndvi_mean\n",
    "    \n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    DF = DF[DF['MIDPOINT_DATETIME'].dt.year == 2023]\n",
    "    \n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    \n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "    \n",
    "    # scaler = StandardScaler()\n",
    "    # DF[features] = scaler.fit_transform(DF[features])\n",
    "    \n",
    "    mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "    \n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "    \n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    val_data = val_data[val_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    \n",
    "    X_train, y_train = train_data[features], train_data[target]\n",
    "    X_val, y_val = val_data[features], val_data[target]\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "    \n",
    "    # Retrieve best hyperparameters and update them for SHAP training\n",
    "    best_params = XGB_hyperparameters[f'{gap_days}']\n",
    "    best_params.update({\"tree_method\": \"gpu_hist\", \"verbosity\": 0})\n",
    "    \n",
    "    # Train the XGBoost model with early stopping\n",
    "    # best_params['n_estimators'] = 200\n",
    "    model = xgb.XGBRegressor(**best_params, random_state=SEED, n_jobs=-1, early_stopping_rounds=40)\n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              verbose=False)\n",
    "    \n",
    "    # Compute SHAP values on the test set\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "    \n",
    "    # Plot SHAP feature importance (Bar Plot)\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, X_train, plot_type=\"bar\", show=False)\n",
    "    plt.title(f\"SHAP Feature Importance (CO2 Flux)\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot SHAP Beeswarm Plot\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, X_train, show=False)\n",
    "    plt.title(f\"SHAP Beeswarm (CO2 Flux)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP Dependence Plot for 'NDVI'\n",
    "plt.figure()\n",
    "plot_feature = preds_after_corr[7]\n",
    "shap.dependence_plot(plot_feature, shap_values, X_train, interaction_index=None, show=False)\n",
    "plt.title(f\"SHAP Dependence Plot of {plot_feature} vs CO2 Flux SHAP Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_after_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Plot 1: Soil Moisture (main), Soil Temperature (color) ===\n",
    "plt.figure(figsize=(8, 5))\n",
    "shap.dependence_plot(\n",
    "    ind='SOIL_MOISTURE',\n",
    "    shap_values=shap_values,\n",
    "    features=X_train,\n",
    "    interaction_index='SOIL_TEMPERATURE',\n",
    "    show=False\n",
    ")\n",
    "plt.title(\"SOIL_MOISTURE colored by SOIL_TEMPERATURE\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Plot 2: Soil Temperature (main), Soil Moisture (color) ===\n",
    "plt.figure(figsize=(8, 5))\n",
    "shap.dependence_plot(\n",
    "    ind='SOIL_TEMPERATURE',\n",
    "    shap_values=shap_values,\n",
    "    features=X_train,\n",
    "    interaction_index='SOIL_MOISTURE',\n",
    "    show=False\n",
    ")\n",
    "plt.title(\"SOIL_TEMPERATURE colored by SOIL_MOISTURE\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    "    ind='NDVI', \n",
    "    shap_values=shap_values, \n",
    "    features=X_train, \n",
    "    interaction_index='INCOMING_SHORTWAVE_RADIATION'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    "    ind='SOIL_MOISTURE',\n",
    "    shap_values=shap_values,\n",
    "    features=X_train,\n",
    "    interaction_index='VAPOUR_PRESSURE_DEFICIT'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define interaction pairs\n",
    "combinations = [\n",
    "    ('NDVI', 'INCOMING_SHORTWAVE_RADIATION'),\n",
    "    ('SOIL_MOISTURE', 'VAPOUR_PRESSURE_DEFICIT'),\n",
    "    ('SOIL_TEMPERATURE', 'DOY'),\n",
    "    ('WIND_SPEED', 'NDVI'),\n",
    "    ('RAINFALL_14DAYS_SUM', 'SOIL_MOISTURE'),\n",
    "    ('VAPOUR_PRESSURE_DEFICIT', 'NDVI'),\n",
    "    ('INCOMING_SHORTWAVE_RADIATION', 'NDVI'),\n",
    "    ('DOY', 'SOIL_TEMPERATURE'),\n",
    "    ('SOIL_HEAT_FLUX', 'SOIL_TEMPERATURE'),\n",
    "    ('Hour_cos', 'INCOMING_SHORTWAVE_RADIATION'),\n",
    "]\n",
    "\n",
    "# Loop and plot\n",
    "for main_feat, color_feat in combinations:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    shap.dependence_plot(ind=main_feat, shap_values=shap_values, features=X_train, interaction_index=color_feat, show=False)\n",
    "    plt.title(f\"{main_feat} colored by {color_feat}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Plot 1: Radiation (main), Soil Temperature (color) ===\n",
    "plt.figure(figsize=(8, 5))\n",
    "shap.dependence_plot(\n",
    "    ind='INCOMING_SHORTWAVE_RADIATION',\n",
    "    shap_values=shap_values,\n",
    "    features=X_train,\n",
    "    interaction_index='SOIL_TEMPERATURE',\n",
    "    show=False\n",
    ")\n",
    "plt.title(\"INCOMING_SHORTWAVE_RADIATION colored by SOIL_TEMPERATURE\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Plot 2: Soil Temperature (main), Radiation (color) ===\n",
    "plt.figure(figsize=(8, 5))\n",
    "shap.dependence_plot(\n",
    "    ind='SOIL_TEMPERATURE',\n",
    "    shap_values=shap_values,\n",
    "    features=X_train,\n",
    "    interaction_index='INCOMING_SHORTWAVE_RADIATION',\n",
    "    show=False\n",
    ")\n",
    "plt.title(\"SOIL_TEMPERATURE colored by INCOMING_SHORTWAVE_RADIATION\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_after_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "LSTM_hyperparameters = {}\n",
    "gap_list = [2, 4, 7, 14, 21, 30]\n",
    "lookback = 1  # Using a lookback of 1 timestep\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    # Prepare the dataset\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "    \n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    \n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "    \n",
    "    mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "    \n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "    \n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    val_data = val_data[val_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    \n",
    "    # Extract features and target arrays\n",
    "    X_train = train_data[features].values\n",
    "    y_train = train_data[target].values\n",
    "    X_val = val_data[features].values\n",
    "    y_val = val_data[target].values\n",
    "    X_test = test_data[features].values\n",
    "    y_test = test_data[target].values\n",
    "    \n",
    "    # Reshape data for LSTM: (samples, timesteps, features)\n",
    "    X_train_seq = X_train.reshape(-1, lookback, len(features))\n",
    "    X_val_seq   = X_val.reshape(-1, lookback, len(features))\n",
    "    X_test_seq  = X_test.reshape(-1, lookback, len(features))\n",
    "    \n",
    "    # Hyperparameter Optimization using Optuna for LSTM\n",
    "    def objective(trial):\n",
    "        units = trial.suggest_int(\"units\", 16, 128, step=16)\n",
    "        dropout_rate = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=units, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(1))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "        \n",
    "        model.fit(X_train_seq, y_train, validation_data=(X_val_seq, y_val),\n",
    "                  epochs=100, batch_size=32, verbose=0, callbacks=[early_stop])\n",
    "        \n",
    "        y_pred = model.predict(X_val_seq)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        return rmse\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=50)  # Fewer trials for faster tuning\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    print(\"Best hyperparameters for gap\", gap_days, \":\", best_params)\n",
    "    LSTM_hyperparameters[f'{gap_days}'] = best_params\n",
    "    \n",
    "    # Train final LSTM model using best hyperparameters\n",
    "    units = best_params[\"units\"]\n",
    "    dropout_rate = best_params[\"dropout\"]\n",
    "    lr = best_params[\"lr\"]\n",
    "    \n",
    "    final_model = Sequential()\n",
    "    final_model.add(LSTM(units=units, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n",
    "    final_model.add(Dropout(dropout_rate))\n",
    "    final_model.add(Dense(1))\n",
    "    \n",
    "    final_model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "    \n",
    "    final_model.fit(X_train_seq, y_train, validation_data=(X_val_seq, y_val),\n",
    "                    epochs=100, batch_size=32, verbose=0, callbacks=[early_stop])\n",
    "    \n",
    "    y_pred_test = final_model.predict(X_test_seq)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    r2 = r2_score(y_test, y_pred_test)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred_test)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(test_data['TIMESTAMP'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    plt.plot(test_data['TIMESTAMP'], y_pred_test, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    plt.axvspan(gap_start, gap_end, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "    plt.title(f\"HW | LSTM | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the PyTorch LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, timesteps, features)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out[:, -1, :])  # Use the output of the last time step\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "LSTM_hyperparameters = {}\n",
    "gap_list = [2, 4, 7, 14, 21, 30]\n",
    "lookback = 1  # each sample is a sequence of 1 timestep\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    # Prepare the dataset\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "    \n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    \n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "    \n",
    "    mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "    \n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "    \n",
    "    # Remove flagged values\n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    val_data = val_data[val_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    \n",
    "    # Extract features and target values as NumPy arrays\n",
    "    X_train = train_data[features].values\n",
    "    y_train = train_data[target].values\n",
    "    X_val = val_data[features].values\n",
    "    y_val = val_data[target].values\n",
    "    X_test = test_data[features].values\n",
    "    y_test = test_data[target].values\n",
    "    \n",
    "    # Reshape to (samples, timesteps, features)\n",
    "    X_train_seq = X_train.reshape(-1, lookback, len(features))\n",
    "    X_val_seq   = X_val.reshape(-1, lookback, len(features))\n",
    "    X_test_seq  = X_test.reshape(-1, lookback, len(features))\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    X_val_tensor = torch.tensor(X_val_seq, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    batch_size = 32\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Define the Optuna objective function for LSTM hyperparameters\n",
    "    def objective(trial):\n",
    "        hidden_size = trial.suggest_int(\"hidden_size\", 16, 128, step=16)\n",
    "        dropout_rate = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "        \n",
    "        model = LSTMModel(input_size=len(features), hidden_size=hidden_size, dropout=dropout_rate).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        max_epochs = 100\n",
    "        patience = 10\n",
    "        best_val_loss = np.inf\n",
    "        epochs_no_improve = 0\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            model.train()\n",
    "            train_losses = []\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(xb)\n",
    "                loss = criterion(outputs, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "            avg_train_loss = np.mean(train_losses)\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    outputs = model(xb)\n",
    "                    loss = criterion(outputs, yb)\n",
    "                    val_losses.append(loss.item())\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    break\n",
    "        \n",
    "        return np.sqrt(best_val_loss)  # Return RMSE on validation\n",
    "    \n",
    "    sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=50)  # Fewer trials for faster tuning\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    print(\"Best hyperparameters for gap\", gap_days, \":\", best_params)\n",
    "    LSTM_hyperparameters[f'{gap_days}'] = best_params\n",
    "    \n",
    "    # Train the final model with best hyperparameters\n",
    "    hidden_size = best_params[\"hidden_size\"]\n",
    "    dropout_rate = best_params[\"dropout\"]\n",
    "    lr = best_params[\"lr\"]\n",
    "    \n",
    "    final_model = LSTMModel(input_size=len(features), hidden_size=hidden_size, dropout=dropout_rate).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(final_model.parameters(), lr=lr)\n",
    "    \n",
    "    max_epochs = 100\n",
    "    patience = 10\n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        final_model.train()\n",
    "        train_losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = final_model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        \n",
    "        final_model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                outputs = final_model(xb)\n",
    "                loss = criterion(outputs, yb)\n",
    "                val_losses.append(loss.item())\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                break\n",
    "    \n",
    "    # Evaluate on the test (gap) period\n",
    "    final_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_test = final_model(X_test_tensor.to(device)).cpu().numpy()\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    r2 = r2_score(y_test, y_pred_test)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred_test)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(test_data['TIMESTAMP'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "    plt.plot(test_data['TIMESTAMP'], y_pred_test, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "    plt.axvspan(gap_start, gap_end, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "    plt.title(f\"HW | LSTM (PyTorch) | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"CO₂ Flux\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_after_corr = ['INCOMING_SHORTWAVE_RADIATION',\n",
    "'OUTGOING_LONGWAVE_RADIATION',\n",
    " 'VAPOUR_PRESSURE_DEFICIT',\n",
    " 'SOIL_MOISTURE',\n",
    " 'SOIL_TEMPERATURE',\n",
    " 'SOIL_HEAT_FLUX',\n",
    " 'RAINFALL_14DAYS_SUM',\n",
    " 'WIND_SPEED',\n",
    " 'DOY',\n",
    " 'Hour_cos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "RF_hyperparameters = {}\n",
    "gap_list = [2, 4, 7, 14, 21, 30]\n",
    "\n",
    "# Define symbolic regression function (same as previous equation)\n",
    "def symbolic_regression_prediction(row):\n",
    "    # Correct mapping based on original equation:\n",
    "    x1 = row[preds_after_corr[0]]  # INCOMING_SHORTWAVE_RADIATION\n",
    "    x2 = row[preds_after_corr[4]]  # SOIL_TEMPERATURE\n",
    "    x4 = row[preds_after_corr[2]]  # VAPOUR_PRESSURE_DEFICIT\n",
    "    x5 = row[preds_after_corr[7]]  # WIND_SPEED\n",
    "    x6 = row[preds_after_corr[3]]  # SOIL_MOISTURE\n",
    "\n",
    "    term1 = 6.28 * x1 + 6.31 * x2 - 3.86 * x4 - 1.14 * x5 + 3.69 * x6\n",
    "    term2 = -141.04 * np.sin(0.13 * x2 - 0.1 * x6 + 5.84 - 0.66 * np.exp(-0.45 * x1))\n",
    "    return term1 + term2 - 128.5\n",
    "\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    # Sample DataFrame (replace with actual dataset)\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "    # Use the given predictors for bias correction (same as symbolic regression inputs)\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "\n",
    "    # Compute Day_flag: 1 for daytime (SW >= 10 W/m²), 0 for nighttime\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "    \n",
    "    # --- NEW: Compute symbolic regression prediction and bias ---\n",
    "    DF['Predicted_Symbolic'] = DF.apply(symbolic_regression_prediction, axis=1)\n",
    "    DF['Bias'] = DF[target] - DF['Predicted_Symbolic']\n",
    "    # -------------------------------------------------------------\n",
    "\n",
    "    # Define Validation and Test Split\n",
    "    mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "\n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    # Define Validation Region (same duration as test, just before it)\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "\n",
    "    # Remove flagged values\n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    val_data = val_data[val_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    # test_data = test_data[test_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "\n",
    "    # --- Change: Use Bias (error) as the target for the Random Forest ---\n",
    "    X_train, y_train = train_data[features], train_data['Bias']\n",
    "    X_val, y_val = val_data[features], val_data['Bias']\n",
    "    # For testing, we use features to predict the bias and then add it to the symbolic prediction\n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data[target]\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    # Hyperparameter Optimization using Optuna\n",
    "    def objective(trial):\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 5)\n",
    "        max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "        \n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        return rmse  # minimize\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=200)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(\"Best hyperparameters for\", gap_days, \"day gap:\", best_params)\n",
    "\n",
    "    RF_hyperparameters[f'{gap_days}'] = best_params\n",
    "\n",
    "    # Train and Evaluate with Optimized Random Forest bias corrector\n",
    "    def train_and_evaluate(X_train, y_train, X_test, y_test, best_params, test_data):\n",
    "        model = RandomForestRegressor(\n",
    "            **best_params, random_state=SEED, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        predicted_bias = model.predict(X_test)\n",
    "        # Add the predicted bias to the symbolic prediction for final output\n",
    "        y_pred_final = test_data['Predicted_Symbolic'] + predicted_bias\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
    "        r2 = r2_score(y_test, y_pred_final)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred_final)\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(test_data['TIMESTAMP'], y_test, color='red', linestyle='dashed', label='Actual (Gap)', marker='o')\n",
    "        plt.plot(test_data['TIMESTAMP'], y_pred_final, color='green', linestyle='dashed', label='Predicted (Gap-filled)', marker='o')\n",
    "        plt.axvspan(gap_start, gap_end, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "        plt.title(f\"HW | Symbolic Regression + RF Bias Correction | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "        plt.xlabel(\"Timestamp\")\n",
    "        plt.ylabel(\"CO₂ Flux\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    train_and_evaluate(X_train, y_train, X_test, y_test, best_params, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEE Equation used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define parameters (use appropriate values from the paper)\n",
    "R_ref = 1.24  # Reference respiration (µmolCO2 m−2 s−1)\n",
    "E_0 = 54       # Temperature sensitivity (Kelvin)\n",
    "T_ref = 283.15  # Reference temperature (Kelvin, 10°C)\n",
    "T_0 = 261      # Temperature where respiration is negligible (Kelvin)\n",
    "GPP_opt = -25.9 # Maximum CO2 uptake when PAR saturates photosynthesis (µmolCO2 m−2 s−1)\n",
    "alpha = 0.0081  # Apparent quantum yield (µmolCO2 µmol photon−1)\n",
    "\n",
    "# Sample DataFrame (replace with actual dataset)\n",
    "DF = DF_HW.copy(deep=True)\n",
    "# DF = pd.read_csv('dataset.csv')\n",
    "DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "# Define necessary environmental variables\n",
    "features = ['INCOMING_SHORTWAVE_RADIATION', 'SOIL_TEMPERATURE']\n",
    "scaler = StandardScaler()\n",
    "DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "# Compute Reco, GPP, and NEE\n",
    "def compute_fluxes(row):\n",
    "    T = row['SOIL_TEMPERATURE'] * scaler.scale_[1] + scaler.mean_[1] + 273.15 # Inverse transform temperature\n",
    "    SW = row['INCOMING_SHORTWAVE_RADIATION'] * scaler.scale_[0] + scaler.mean_[0]  # Inverse transform SW\n",
    "    PAR = SW * 2.02  # Convert SW to PAR # SW in W/m2\n",
    "    \n",
    "    # Ecosystem Respiration (Reco)\n",
    "    Reco = R_ref * np.exp(E_0 * ((1 / (T_ref - T_0)) - (1 / (T - T_0))))\n",
    "    \n",
    "    # Gross Primary Production (GPP)\n",
    "    GPP = alpha*(GPP_opt * PAR) / (alpha*PAR + GPP_opt)\n",
    "    \n",
    "    # Net Ecosystem Exchange (NEE)\n",
    "    NEE = Reco - GPP\n",
    "    \n",
    "    return pd.Series([Reco, GPP, NEE])\n",
    "\n",
    "\n",
    "DF[['Reco_newly_calculated', 'GPP_newly_calculated', 'Predicted_NEE']] = DF.apply(compute_fluxes, axis=1)\n",
    "\n",
    "# Calculate RMSE, R², and MAPE\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    mape = mean_absolute_percentage_error(true_values, predicted_values)\n",
    "    return rmse, r2, mape\n",
    "\n",
    "# Plot results\n",
    "gap_sizes = [2, 4, 7, 14, 21, 30]\n",
    "fig, axes = plt.subplots(len(gap_sizes), 1, figsize=(12, 3 * len(gap_sizes)))\n",
    "\n",
    "def plot_gap(DF, gap_days, ax):\n",
    "    mid_timestamp = DF['TIMESTAMP'].quantile(0.4)\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "\n",
    "    train_data = DF[(DF['TIMESTAMP'] < gap_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "    actual_values = test_data['NET_CARBON_DIOXIDE_FLUX']\n",
    "    predicted_values = test_data['Predicted_NEE']\n",
    "    \n",
    "    rmse, r2, mape = calculate_metrics(actual_values, predicted_values)\n",
    "\n",
    "    ax.plot(test_data['TIMESTAMP'], actual_values, color='red', linestyle='dashed', label='Actual (Gap)')\n",
    "    ax.plot(test_data['TIMESTAMP'], predicted_values, color='green', marker='o', linestyle='dashed', label='Predicted (Gap-filled)')\n",
    "    ax.axvspan(gap_start, gap_end, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "    ax.set_title(f\"NEE Equation(Default Coefficients) | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    ax.set_xlabel(\"Timestamp\")\n",
    "    ax.set_ylabel(\"CO₂ Flux\")\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "for i, gap in enumerate(gap_sizes):\n",
    "    plot_gap(DF, gap, axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEE + Error Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # Sample DataFrame (replace with your data)\n",
    "# DF = DF_HW.copy(deep=True)\n",
    "# DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "# # Define features and target\n",
    "# # features = ['INCOMING_SHORTWAVE_RADIATION', 'SOIL_TEMPERATURE', 'AIR_PRESSURE', \n",
    "# #             'VAPOUR_PRESSURE_DEFICIT', 'WIND_SPEED', 'SOIL_MOISTURE', 'SOIL_HEAT_FLUX', 'DOY']\n",
    "# features = preds_after_corr\n",
    "\n",
    "# target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "\n",
    "# # Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "# # Define symbolic regression equation\n",
    "# # Define parameters\n",
    "# R_ref = 1.24\n",
    "# E_0 = 54\n",
    "# T_ref = 283.15\n",
    "# T_0 = 261\n",
    "# GPP_opt = -25.9\n",
    "# alpha = 0.0081\n",
    "\n",
    "\n",
    "# # Compute Reco, GPP, and NEE\n",
    "# def compute_fluxes(row):\n",
    "#     T = row['SOIL_TEMPERATURE'] * scaler.scale_[1] + scaler.mean_[1] + 273.15\n",
    "#     SW = row['INCOMING_SHORTWAVE_RADIATION'] * scaler.scale_[0] + scaler.mean_[0]\n",
    "#     PAR = SW * 2.02  \n",
    "    \n",
    "#     Reco = R_ref * np.exp(E_0 * ((1 / (T_ref - T_0)) - (1 / (T - T_0))))\n",
    "#     GPP = alpha * (GPP_opt * PAR) / (alpha * PAR + GPP_opt)\n",
    "#     NEE = Reco - GPP\n",
    "    \n",
    "#     return pd.Series([Reco, GPP, NEE])\n",
    "\n",
    "# # Compute residual (bias) from symbolic regression\n",
    "\n",
    "# DF[['Reco_newly_calculated', 'GPP_newly_calculated', 'Predicted_Symbolic']] = DF.apply(compute_fluxes, axis=1)\n",
    "# DF['Bias'] = DF[target] - DF['Predicted_Symbolic']\n",
    "\n",
    "# # Pre-split data for initial bias correction training (optional, not used in plotting)\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(DF[features], DF['Bias'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # Best parameters for the Random Forest (can be tuned separately)\n",
    "# best_params_rf = {\n",
    "#     'max_depth': 14,\n",
    "#     'max_features': 0.5030123021686415,\n",
    "#     'min_samples_leaf': 2,\n",
    "#     'min_samples_split': 4,\n",
    "#     'n_estimators': 264\n",
    "# }\n",
    "\n",
    "# # Function to compute metrics\n",
    "# def calculate_metrics(true_values, predicted_values):\n",
    "#     rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "#     r2 = r2_score(true_values, predicted_values)\n",
    "#     mape = mean_absolute_percentage_error(true_values, predicted_values)\n",
    "#     return rmse, r2, mape\n",
    "\n",
    "# # Plot results for different gap sizes\n",
    "# gap_sizes = [2, 4, 7, 14, 21, 30]\n",
    "# fig, axes = plt.subplots(len(gap_sizes), 1, figsize=(12, 3 * len(gap_sizes)))\n",
    "\n",
    "# def plot_gap(DF, gap_days, ax):\n",
    "#     # Define gap period based on quantile of timestamp\n",
    "#     mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "#     gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "#     gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    \n",
    "#     # Split data: training (outside gap) and testing (inside gap)\n",
    "#     train_data = DF[(DF['TIMESTAMP'] < gap_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "#     test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "    \n",
    "#     # Retrain the Random Forest bias corrector on training data to avoid leakage\n",
    "#     X_train_gap = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0][features]\n",
    "#     y_train_gap = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]['Bias']\n",
    "#     rf_gap = RandomForestRegressor(**best_params_rf, n_jobs=32, random_state=42)\n",
    "#     rf_gap.fit(X_train_gap, y_train_gap)\n",
    "    \n",
    "#     # Predict bias correction on test data using the model trained on train_data\n",
    "#     test_data = test_data[test_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0].copy()\n",
    "#     test_data['Bias_Corrected'] = rf_gap.predict(test_data[features])\n",
    "#     test_data['Predicted_Final'] = test_data['Predicted_Symbolic'] + test_data['Bias_Corrected']\n",
    "    \n",
    "#     actual_values = test_data[target]\n",
    "#     predicted_values = test_data['Predicted_Final']\n",
    "    \n",
    "#     rmse, r2, mape = calculate_metrics(actual_values, predicted_values)\n",
    "    \n",
    "#     ax.plot(test_data['TIMESTAMP'], actual_values, color='red',marker='o', linestyle='dashed', label='Actual (Gap)')\n",
    "#     ax.plot(test_data['TIMESTAMP'], predicted_values, color='green', marker='o', linestyle='dashed', label='Predicted (Gap-filled)')\n",
    "#     ax.axvspan(gap_start, gap_end, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "    \n",
    "#     ax.set_title(f\"FNE | NEE Equation + RF Error Correction | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "#     ax.set_xlabel(\"Timestamp\")\n",
    "#     ax.set_ylabel(\"CO₂ Flux, micro-mol CO2 /m2\")\n",
    "#     ax.legend()\n",
    "#     ax.grid()\n",
    "\n",
    "# # Plot for different gap sizes\n",
    "# for i, gap in enumerate(gap_sizes):\n",
    "#     plot_gap(DF, gap, axes[i])\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "RF_hyperparameters = {}\n",
    "# gap_list = [2, 4, 7, 14, 21, 30]\n",
    "gap_list = [21]\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    # Sample DataFrame (replace with actual dataset)\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "    \n",
    "    # Define features and target\n",
    "    # Here, preds_after_corr is assumed to be defined externally (e.g., a list of feature names)\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    \n",
    "    # Compute Day_flag: 1 for daytime (SW >= 10 W/m²), 0 for nighttime\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute the prediction from the symbolic (physical) equation.\n",
    "    # Define parameters for the physical equation\n",
    "    R_ref = 1.24\n",
    "    E_0 = 54\n",
    "    T_ref = 283.15\n",
    "    T_0 = 261\n",
    "    GPP_opt = -25.9\n",
    "    alpha = 0.0081\n",
    "    \n",
    "    # We need to de-standardize the features used by the equation.\n",
    "    # Here we assume that the required variables (\"INCOMING_SHORTWAVE_RADIATION\" and \"SOIL_TEMPERATURE\")\n",
    "    # are present in 'features'. We find their index in the list.\n",
    "    idx_sw = features.index('INCOMING_SHORTWAVE_RADIATION')\n",
    "    idx_temp = features.index('SOIL_TEMPERATURE')\n",
    "    \n",
    "    def compute_fluxes(row):\n",
    "        # De-standardize SOIL_TEMPERATURE and INCOMING_SHORTWAVE_RADIATION\n",
    "        T_original = row['SOIL_TEMPERATURE'] * scaler.scale_[idx_temp] + scaler.mean_[idx_temp]\n",
    "        T = T_original + 273.15\n",
    "        SW_original = row['INCOMING_SHORTWAVE_RADIATION'] * scaler.scale_[idx_sw] + scaler.mean_[idx_sw]\n",
    "        PAR = SW_original * 2.02  \n",
    "        Reco = R_ref * np.exp(E_0 * ((1 / (T_ref - T_0)) - (1 / (T - T_0))))\n",
    "        GPP = alpha * (GPP_opt * PAR) / (alpha * PAR + GPP_opt)\n",
    "        NEE = Reco - GPP\n",
    "        return NEE\n",
    "    \n",
    "    DF['Predicted_Symbolic'] = DF.apply(compute_fluxes, axis=1)\n",
    "    \n",
    "    # Compute bias as the error between the actual target and the symbolic prediction\n",
    "    DF['Bias'] = DF[target] - DF['Predicted_Symbolic']\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # (Note: In bias correction, we use the original features only)\n",
    "    \n",
    "    # Define Validation and Test Split based on timestamps\n",
    "    mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    \n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "    \n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    # Define Validation Region (same duration as test, just before it)\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "    \n",
    "    # Remove flagged values (quality flag filter)\n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    val_data = val_data[val_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    # Optionally, filter test_data if needed.\n",
    "    \n",
    "    # For bias correction, the RF is trained to predict Bias using original features.\n",
    "    X_train, y_train = train_data[features], train_data['Bias']\n",
    "    X_val, y_val = val_data[features], val_data['Bias']\n",
    "    # For testing, we still need the actual target for evaluation;\n",
    "    # note that final prediction is the sum of the equation and predicted bias.\n",
    "    X_test, y_test = test_data[features], test_data[target]\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Hyperparameter Optimization using Optuna for bias correction\n",
    "    def objective(trial):\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 5)\n",
    "        max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "        \n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        return rmse  # minimize\n",
    "    \n",
    "    sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=200)\n",
    "    \n",
    "    # Print best hyperparameters\n",
    "    best_params = study.best_params\n",
    "    print(\"Best hyperparameters for gap\", gap_days, \":\", best_params)\n",
    "    RF_hyperparameters[f'{gap_days}'] = best_params\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Train and Evaluate with Optimized Random Forest (bias corrector)\n",
    "    def train_and_evaluate(X_train, y_train, X_test, y_test, best_params):\n",
    "        model = RandomForestRegressor(\n",
    "            **best_params, random_state=SEED, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        # Predict bias on the test set\n",
    "        bias_pred = model.predict(X_test)\n",
    "        # Final prediction is the symbolic prediction plus the predicted bias\n",
    "        y_pred_final = test_data['Predicted_Symbolic'] + bias_pred\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
    "        r2 = r2_score(y_test, y_pred_final)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred_final)\n",
    "        \n",
    "        # Plot results\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(test_data['TIMESTAMP'], y_test, color='red', linestyle='dashed',\n",
    "                 label='Actual (Gap)', marker='o')\n",
    "        plt.plot(test_data['TIMESTAMP'], y_pred_final, color='green', linestyle='dashed',\n",
    "                 label='Predicted (Gap-filled)', marker='o')\n",
    "        plt.axvspan(gap_start, gap_end, color='gray', alpha=0.2,\n",
    "                    label=f'{gap_days}-Day Gap')\n",
    "        plt.title(f\"HW | NEE Equation + RF Bias Correction | {gap_days}-Day Gap | \"\n",
    "                  f\"R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "        plt.xlabel(\"Timestamp\")\n",
    "        plt.ylabel(\"CO₂ Flux\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    train_and_evaluate(X_train, y_train, X_test, y_test, best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEE prediction as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Sample DataFrame (replace with your data)\n",
    "DF = DF_HW.copy(deep=True)\n",
    "DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "# Define original features and target\n",
    "features = ['INCOMING_SHORTWAVE_RADIATION', 'SOIL_TEMPERATURE', 'AIR_PRESSURE', \n",
    "            'VAPOUR_PRESSURE_DEFICIT', 'WIND_SPEED', 'SOIL_MOISTURE', 'SOIL_HEAT_FLUX', 'DOY']\n",
    "target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "# Define symbolic regression equation parameters\n",
    "R_ref = 1.24\n",
    "E_0 = 54\n",
    "T_ref = 283.15\n",
    "T_0 = 261\n",
    "GPP_opt = -25.9\n",
    "alpha = 0.0081\n",
    "\n",
    "# Compute Reco, GPP, and NEE using the physical equation\n",
    "def compute_fluxes(row):\n",
    "    T = row['SOIL_TEMPERATURE'] * scaler.scale_[1] + scaler.mean_[1] + 273.15\n",
    "    SW = row['INCOMING_SHORTWAVE_RADIATION'] * scaler.scale_[0] + scaler.mean_[0]\n",
    "    PAR = SW * 2.02  \n",
    "    Reco = R_ref * np.exp(E_0 * ((1 / (T_ref - T_0)) - (1 / (T - T_0))))\n",
    "    GPP = alpha * (GPP_opt * PAR) / (alpha * PAR + GPP_opt)\n",
    "    NEE = Reco - GPP\n",
    "    return pd.Series([Reco, GPP, NEE])\n",
    "\n",
    "DF[['Reco_newly_calculated', 'GPP_newly_calculated', 'Predicted_Symbolic']] = DF.apply(compute_fluxes, axis=1)\n",
    "\n",
    "# Augment the RF feature set to include the symbolic equation prediction\n",
    "features_rf = features + ['Predicted_Symbolic']\n",
    "\n",
    "# Best parameters for the Random Forest (can be tuned separately)\n",
    "best_params_rf = {\n",
    "    'max_depth': 14,\n",
    "    'max_features': 0.5030123021686415,\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 4,\n",
    "    'n_estimators': 264\n",
    "}\n",
    "\n",
    "# Function to compute metrics\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    mape = mean_absolute_percentage_error(true_values, predicted_values)\n",
    "    return rmse, r2, mape\n",
    "\n",
    "# Plot results for different gap sizes\n",
    "gap_sizes = [2, 4, 7, 14, 21, 30]\n",
    "fig, axes = plt.subplots(len(gap_sizes), 1, figsize=(12, 3 * len(gap_sizes)))\n",
    "\n",
    "def plot_gap(DF, gap_days, ax):\n",
    "    # Define gap period based on quantile of timestamp\n",
    "    mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    \n",
    "    # Split data: training (outside gap) and testing (inside gap)\n",
    "    train_data = DF[(DF['TIMESTAMP'] < gap_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "    \n",
    "    # Filter for valid data points using the quality flag\n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    test_data = test_data[test_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0].copy()\n",
    "    \n",
    "    # Train the RF to directly predict the target using the augmented feature set\n",
    "    X_train_gap = train_data[features_rf]\n",
    "    y_train_gap = train_data[target]\n",
    "    rf_gap = RandomForestRegressor(**best_params_rf, n_jobs=32, random_state=42)\n",
    "    rf_gap.fit(X_train_gap, y_train_gap)\n",
    "    \n",
    "    # Predict target on test data\n",
    "    X_test_gap = test_data[features_rf]\n",
    "    test_data['Predicted_Final'] = rf_gap.predict(X_test_gap)\n",
    "    \n",
    "    actual_values = test_data[target]\n",
    "    predicted_values = test_data['Predicted_Final']\n",
    "    \n",
    "    rmse, r2, mape = calculate_metrics(actual_values, predicted_values)\n",
    "    \n",
    "    ax.plot(test_data['TIMESTAMP'], actual_values, color='red', marker='o', linestyle='dashed', label='Actual (Gap)')\n",
    "    ax.plot(test_data['TIMESTAMP'], predicted_values, color='green', marker='o', linestyle='dashed', label='Predicted (Gap-filled)')\n",
    "    ax.axvspan(gap_start, gap_end, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "    \n",
    "    ax.set_title(f\"HW | NEE Equation + RF (with Equation as Feature) | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    ax.set_xlabel(\"Timestamp\")\n",
    "    ax.set_ylabel(\"CO₂ Flux, micro-mol CO₂/m²\")\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "# Plot for different gap sizes\n",
    "for i, gap in enumerate(gap_sizes):\n",
    "    plot_gap(DF, gap, axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_after_corr = ['INCOMING_SHORTWAVE_RADIATION',\n",
    " 'OUTGOING_LONGWAVE_RADIATION',\n",
    " 'VAPOUR_PRESSURE_DEFICIT',\n",
    " 'SOIL_MOISTURE',\n",
    " 'SOIL_TEMPERATURE',\n",
    " 'SOIL_HEAT_FLUX',\n",
    " 'RAINFALL_14DAYS_SUM',\n",
    " 'WIND_SPEED',\n",
    " 'DOY',\n",
    " 'Hour_cos']\n",
    "\n",
    "\n",
    "features_subset = [\n",
    "'OUTGOING_LONGWAVE_RADIATION',\n",
    " 'VAPOUR_PRESSURE_DEFICIT',\n",
    " 'SOIL_MOISTURE',\n",
    " 'SOIL_HEAT_FLUX',\n",
    " 'RAINFALL_14DAYS_SUM',\n",
    " 'WIND_SPEED',\n",
    " 'DOY',\n",
    " 'Hour_cos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "RF_hyperparameters = {}\n",
    "gap_list = [2, 4, 7, 14, 21, 30]\n",
    "\n",
    "for gap_days in gap_list:\n",
    "    # Sample DataFrame (replace with actual dataset)\n",
    "    DF = DF_HW.copy(deep=True)\n",
    "    DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "    \n",
    "    # Define features and target\n",
    "    # Here, preds_after_corr is assumed to be defined externally (e.g., a list of feature names)\n",
    "    features = preds_after_corr\n",
    "    target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "    \n",
    "    # Compute Day_flag: 1 for daytime (SW >= 10 W/m²), 0 for nighttime\n",
    "    DF['Day_flag'] = (DF['INCOMING_SHORTWAVE_RADIATION'] >= 10).astype(float)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    DF[features] = scaler.fit_transform(DF[features])\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute the prediction from the symbolic (physical) equation.\n",
    "    # Define parameters for the physical equation\n",
    "    R_ref = 1.24\n",
    "    E_0 = 54\n",
    "    T_ref = 283.15\n",
    "    T_0 = 261\n",
    "    GPP_opt = -25.9\n",
    "    alpha = 0.0081\n",
    "    \n",
    "    # We need to de-standardize the features used by the equation.\n",
    "    # Here we assume that the required variables (\"INCOMING_SHORTWAVE_RADIATION\" and \"SOIL_TEMPERATURE\")\n",
    "    # are present in 'features'. We find their index in the list.\n",
    "    idx_sw = features.index('INCOMING_SHORTWAVE_RADIATION')\n",
    "    idx_temp = features.index('SOIL_TEMPERATURE')\n",
    "    \n",
    "    def compute_fluxes(row):\n",
    "        # De-standardize SOIL_TEMPERATURE and INCOMING_SHORTWAVE_RADIATION\n",
    "        T_original = row['SOIL_TEMPERATURE'] * scaler.scale_[idx_temp] + scaler.mean_[idx_temp]\n",
    "        T = T_original + 273.15\n",
    "        SW_original = row['INCOMING_SHORTWAVE_RADIATION'] * scaler.scale_[idx_sw] + scaler.mean_[idx_sw]\n",
    "        PAR = SW_original * 2.02  \n",
    "        Reco = R_ref * np.exp(E_0 * ((1 / (T_ref - T_0)) - (1 / (T - T_0))))\n",
    "        GPP = alpha * (GPP_opt * PAR) / (alpha * PAR + GPP_opt)\n",
    "        NEE = Reco - GPP\n",
    "        return NEE\n",
    "    \n",
    "    DF['Predicted_Symbolic'] = DF.apply(compute_fluxes, axis=1)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Augment the feature set by including the symbolic prediction\n",
    "    features_aug = features + ['Predicted_Symbolic']\n",
    "    \n",
    "    # Define Validation and Test Split based on timestamps\n",
    "    mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    \n",
    "    val_end = gap_start\n",
    "    val_start = val_end - (gap_end - gap_start)\n",
    "    \n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    # Define Validation Region (same duration as test, just before it)\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "    \n",
    "    # Remove flagged values (quality flag filter)\n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    val_data = val_data[val_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    # Optionally, you can filter test_data if needed.\n",
    "    \n",
    "    # Extract features (augmented) and target for training, validation, and testing\n",
    "    X_train, y_train = train_data[features_aug], train_data[target]\n",
    "    X_val, y_val = val_data[features_aug], val_data[target]\n",
    "    X_test, y_test = test_data[features_aug], test_data[target]\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Hyperparameter Optimization using Optuna\n",
    "    def objective(trial):\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 5)\n",
    "        max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "        \n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        return rmse  # minimize\n",
    "    \n",
    "    sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=200)\n",
    "    \n",
    "    # Print best hyperparameters\n",
    "    best_params = study.best_params\n",
    "    print(\"Best hyperparameters for gap\", gap_days, \":\", best_params)\n",
    "    RF_hyperparameters[f'{gap_days}'] = best_params\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Train and Evaluate with Optimized Random Forest using the augmented feature set\n",
    "    def train_and_evaluate(X_train, y_train, X_test, y_test, best_params):\n",
    "        model = RandomForestRegressor(\n",
    "            **best_params, random_state=SEED, n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        \n",
    "        # Plot results\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(test_data['TIMESTAMP'], y_test, color='red', linestyle='dashed',\n",
    "                 label='Actual (Gap)', marker='o')\n",
    "        plt.plot(test_data['TIMESTAMP'], y_pred, color='green', linestyle='dashed',\n",
    "                 label='Predicted (Gap-filled)', marker='o')\n",
    "        plt.axvspan(gap_start, gap_end, color='gray', alpha=0.2,\n",
    "                    label=f'{gap_days}-Day Gap')\n",
    "        plt.title(f\"HW | RF (with Equation as Feature) | {gap_days}-Day Gap | \"\n",
    "                  f\"R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "        plt.xlabel(\"Timestamp\")\n",
    "        plt.ylabel(\"CO₂ Flux\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    train_and_evaluate(X_train, y_train, X_test, y_test, best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEE Equation as input(Latest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assume DF_HW is defined and contains your dataset\n",
    "DF = DF_HW.copy(deep=True)\n",
    "DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "# Define original features and target\n",
    "features = ['INCOMING_SHORTWAVE_RADIATION', 'SOIL_TEMPERATURE', 'AIR_PRESSURE', \n",
    "            'VAPOUR_PRESSURE_DEFICIT', 'WIND_SPEED', 'SOIL_MOISTURE', 'SOIL_HEAT_FLUX', 'DOY']\n",
    "target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "# Define symbolic regression equation parameters\n",
    "R_ref = 1.24\n",
    "E_0 = 54\n",
    "T_ref = 283.15\n",
    "T_0 = 261\n",
    "GPP_opt = -25.9\n",
    "alpha = 0.0081\n",
    "\n",
    "# Compute fluxes (Reco, GPP, and NEE) using the physical equation.\n",
    "def compute_fluxes(row):\n",
    "    # Recover original values using scaler parameters.\n",
    "    T = row['SOIL_TEMPERATURE'] * scaler.scale_[1] + scaler.mean_[1] + 273.15\n",
    "    SW = row['INCOMING_SHORTWAVE_RADIATION'] * scaler.scale_[0] + scaler.mean_[0]\n",
    "    PAR = SW * 2.02  \n",
    "    Reco = R_ref * np.exp(E_0 * ((1 / (T_ref - T_0)) - (1 / (T - T_0))))\n",
    "    GPP = alpha * (GPP_opt * PAR) / (alpha * PAR + GPP_opt)\n",
    "    NEE = Reco - GPP\n",
    "    return pd.Series([Reco, GPP, NEE])\n",
    "\n",
    "DF[['Reco_newly_calculated', 'GPP_newly_calculated', 'Predicted_Symbolic']] = DF.apply(compute_fluxes, axis=1)\n",
    "\n",
    "# Augment RF feature set with the symbolic equation prediction (NEE)\n",
    "features_rf = features + ['Predicted_Symbolic']\n",
    "\n",
    "# Starting guess for the Random Forest hyperparameters\n",
    "best_params_rf = {\n",
    "    'n_estimators': 264,\n",
    "    'max_depth': 14,\n",
    "    'min_samples_split': 4,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features': 0.5030123021686415\n",
    "}\n",
    "\n",
    "# Function to compute evaluation metrics\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    mape = mean_absolute_percentage_error(true_values, predicted_values)\n",
    "    return rmse, r2, mape\n",
    "\n",
    "# Function to perform tuning and plotting for a given gap size.\n",
    "def plot_and_tune_gap(DF, gap_days, ax):\n",
    "    # Define gap period based on the 55th percentile of timestamps\n",
    "    mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    \n",
    "    # Define a validation period (of the same duration as the gap) immediately preceding the gap.\n",
    "    val_end = gap_start\n",
    "    gap_duration = gap_end - gap_start\n",
    "    val_start = val_end - gap_duration\n",
    "    \n",
    "    # Split data into training, validation, and test sets.\n",
    "    train_data = DF[(DF['TIMESTAMP'] < val_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    val_data = DF[(DF['TIMESTAMP'] >= val_start) & (DF['TIMESTAMP'] < val_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "    \n",
    "    # Remove flagged data from training and validation (and test if needed)\n",
    "    train_data = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    val_data = val_data[val_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]\n",
    "    # test_data = test_data[test_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0].copy()\n",
    "    \n",
    "    X_train = train_data[features_rf]\n",
    "    y_train = train_data[target]\n",
    "    X_val = val_data[features_rf]\n",
    "    y_val = val_data[target]\n",
    "    X_test = test_data[features_rf]\n",
    "    y_test = test_data[target]\n",
    "    \n",
    "    # Define the objective for Optuna hyperparameter tuning.\n",
    "    def objective(trial):\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 5)\n",
    "        max_features = trial.suggest_float(\"max_features\", 0.1, 1.0)\n",
    "        \n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        return rmse\n",
    "    \n",
    "    # Create the Optuna study and enqueue the provided hyperparameters as a starting point.\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.enqueue_trial(best_params_rf)\n",
    "    study.optimize(objective, n_trials=200)\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # Train the final model on the training set with the tuned hyperparameters.\n",
    "    final_model = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)\n",
    "    final_model.fit(X_train, y_train)\n",
    "    y_pred_test = final_model.predict(X_test)\n",
    "    \n",
    "    rmse, r2, mape = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # Plot actual vs. predicted values.\n",
    "    ax.plot(test_data['TIMESTAMP'], y_test, color='red', marker='o', linestyle='dashed', label='Actual (Gap)')\n",
    "    ax.plot(test_data['TIMESTAMP'], y_pred_test, color='green', marker='o', linestyle='dashed', label='Predicted (Gap-filled)')\n",
    "    ax.axvspan(gap_start, gap_end, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "    ax.set_title(f\"HW | RF(NEE Equation prediction as input) | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    ax.set_xlabel(\"Timestamp\")\n",
    "    ax.set_ylabel(\"CO₂ Flux, micro-mol CO₂/m²\")\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "# Define the gap sizes to evaluate.\n",
    "gap_sizes = [2, 4, 7, 14, 21, 30]\n",
    "fig, axes = plt.subplots(len(gap_sizes), 1, figsize=(12, 3 * len(gap_sizes)))\n",
    "\n",
    "for i, gap in enumerate(gap_sizes):\n",
    "    plot_and_tune_gap(DF, gap, axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assume DF_HW is defined and contains your dataset\n",
    "DF = DF_HW.copy(deep=True)\n",
    "DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "# Define original features and target\n",
    "features = ['INCOMING_SHORTWAVE_RADIATION', 'SOIL_TEMPERATURE', 'AIR_PRESSURE', \n",
    "            'VAPOUR_PRESSURE_DEFICIT', 'WIND_SPEED', 'SOIL_MOISTURE', 'SOIL_HEAT_FLUX', 'DOY']\n",
    "target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "# Define symbolic regression equation parameters\n",
    "R_ref = 1.24\n",
    "E_0 = 54\n",
    "T_ref = 283.15\n",
    "T_0 = 261\n",
    "GPP_opt = -25.9\n",
    "alpha = 0.0081\n",
    "\n",
    "# Compute fluxes (Reco, GPP, and NEE) using the physical equation.\n",
    "def compute_fluxes(row):\n",
    "    # Recover original values using scaler parameters.\n",
    "    T = row['SOIL_TEMPERATURE'] * scaler.scale_[1] + scaler.mean_[1] + 273.15\n",
    "    SW = row['INCOMING_SHORTWAVE_RADIATION'] * scaler.scale_[0] + scaler.mean_[0]\n",
    "    PAR = SW * 2.02  \n",
    "    Reco = R_ref * np.exp(E_0 * ((1 / (T_ref - T_0)) - (1 / (T - T_0))))\n",
    "    GPP = alpha * (GPP_opt * PAR) / (alpha * PAR + GPP_opt)\n",
    "    NEE = Reco - GPP\n",
    "    return pd.Series([Reco, GPP, NEE])\n",
    "\n",
    "DF[['Reco_newly_calculated', 'GPP_newly_calculated', 'Predicted_Symbolic']] = DF.apply(compute_fluxes, axis=1)\n",
    "\n",
    "# Augment RF feature set with the symbolic equation prediction (NEE)\n",
    "features_rf = features + ['Predicted_Symbolic']\n",
    "\n",
    "# Starting guess for the Random Forest hyperparameters\n",
    "best_params_rf = {\n",
    "    'n_estimators': 264,\n",
    "    'max_depth': 14,\n",
    "    'min_samples_split': 4,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features': 0.5030123021686415\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.shape[0]*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Enable interactive SHAP plots\n",
    "shap.initjs()\n",
    "\n",
    "# 1. Prepare Data & Split into Train-Test\n",
    "X = DF[features_rf].copy()\n",
    "y = DF[target].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Train RandomForestRegressor\n",
    "final_model = RandomForestRegressor(\n",
    "    **best_params_rf,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Enable parallel processing\n",
    ")\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Evaluate Model\n",
    "y_pred = final_model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Model R² Score on Test Set: {r2:.4f}\")\n",
    "\n",
    "# 4. Use SHAP Explainer\n",
    "explainer = shap.Explainer(final_model)\n",
    "\n",
    "# 5. Compute SHAP values on the entire test set\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# 6. Plot SHAP Summary (Bar & Beeswarm)\n",
    "shap.summary_plot(shap_values, X_test, plot_type='bar')\n",
    "shap.summary_plot(shap_values, X_test, plot_type='dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP with XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Enable interactive SHAP plots\n",
    "shap.initjs()\n",
    "\n",
    "# Load Data\n",
    "X = DF[features_rf].copy()\n",
    "y = DF[target].copy()\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define Optuna Objective Function\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1, 5),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 5),\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(\n",
    "        **params, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Run Optuna Tuning\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "# Get Best Parameters\n",
    "best_params_xgb = study.best_params\n",
    "print(\"Best Parameters:\", best_params_xgb)\n",
    "\n",
    "# Train Final Model with Best Params\n",
    "final_model = xgb.XGBRegressor(\n",
    "    **best_params_xgb, random_state=42, n_jobs=-1\n",
    ")\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = final_model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Model R² Score on Test Set: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute SHAP Values\n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Plot SHAP Summary\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEE*(1+rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Sample DataFrame (replace with your data)\n",
    "DF = DF_HW.copy(deep=True)\n",
    "DF['TIMESTAMP'] = pd.to_datetime(DF['TIMESTAMP'])\n",
    "\n",
    "# Define features and target\n",
    "features = ['INCOMING_SHORTWAVE_RADIATION', 'SOIL_TEMPERATURE', 'AIR_PRESSURE', \n",
    "            'VAPOUR_PRESSURE_DEFICIT', 'WIND_SPEED', 'SOIL_MOISTURE', 'SOIL_HEAT_FLUX', 'DOY']\n",
    "target = 'NET_CARBON_DIOXIDE_FLUX'\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "DF[features] = scaler.fit_transform(DF[features])\n",
    "\n",
    "# Define symbolic regression equation parameters for NEE\n",
    "R_ref = 1.24\n",
    "E_0 = 54\n",
    "T_ref = 283.15\n",
    "T_0 = 261\n",
    "GPP_opt = -25.9\n",
    "alpha = 0.0081\n",
    "\n",
    "# Compute Reco, GPP, and NEE using the physical equation\n",
    "def compute_fluxes(row):\n",
    "    # De-standardize the required variables:\n",
    "    T = row['SOIL_TEMPERATURE'] * scaler.scale_[1] + scaler.mean_[1] + 273.15\n",
    "    SW = row['INCOMING_SHORTWAVE_RADIATION'] * scaler.scale_[0] + scaler.mean_[0]\n",
    "    PAR = SW * 2.02  \n",
    "    Reco = R_ref * np.exp(E_0 * ((1 / (T_ref - T_0)) - (1 / (T - T_0))))\n",
    "    GPP = alpha * (GPP_opt * PAR) / (alpha * PAR + GPP_opt)\n",
    "    NEE = Reco - GPP\n",
    "    return pd.Series([Reco, GPP, NEE])\n",
    "\n",
    "DF[['Reco_newly_calculated', 'GPP_newly_calculated', 'Predicted_Symbolic']] = DF.apply(compute_fluxes, axis=1)\n",
    "# Instead of bias, compute multiplicative gain:\n",
    "DF['Gain'] = DF[target] / DF['Predicted_Symbolic'] - 1\n",
    "\n",
    "# Best parameters for the Random Forest (can be tuned separately)\n",
    "best_params_rf = {\n",
    "    'max_depth': 14,\n",
    "    'max_features': 0.5030123021686415,\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 4,\n",
    "    'n_estimators': 264\n",
    "}\n",
    "\n",
    "# Function to compute metrics\n",
    "def calculate_metrics(true_values, predicted_values):\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    mape = mean_absolute_percentage_error(true_values, predicted_values)\n",
    "    return rmse, r2, mape\n",
    "\n",
    "# Plot results for different gap sizes\n",
    "gap_sizes = [2, 4, 7, 14, 21, 30]\n",
    "fig, axes = plt.subplots(len(gap_sizes), 1, figsize=(12, 3 * len(gap_sizes)))\n",
    "\n",
    "def plot_gap(DF, gap_days, ax):\n",
    "    # Define gap period based on quantile of timestamp\n",
    "    mid_timestamp = DF['TIMESTAMP'].quantile(0.55)\n",
    "    gap_start = mid_timestamp - pd.Timedelta(days=gap_days // 2)\n",
    "    gap_end = mid_timestamp + pd.Timedelta(days=gap_days // 2)\n",
    "    \n",
    "    # Split data: training (outside gap) and testing (inside gap)\n",
    "    train_data = DF[(DF['TIMESTAMP'] < gap_start) | (DF['TIMESTAMP'] > gap_end)]\n",
    "    test_data = DF[(DF['TIMESTAMP'] >= gap_start) & (DF['TIMESTAMP'] <= gap_end)]\n",
    "    \n",
    "    # Retrain the Random Forest gain predictor on training data to avoid leakage\n",
    "    X_train_gap = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0][features]\n",
    "    y_train_gap = train_data[train_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0]['Gain']\n",
    "    rf_gap = RandomForestRegressor(**best_params_rf, n_jobs=32, random_state=42)\n",
    "    rf_gap.fit(X_train_gap, y_train_gap)\n",
    "    \n",
    "    # Predict gain on test data using the model trained on train_data\n",
    "    test_data = test_data[test_data['NET_CARBON_DIOXIDE_FLUX_FLAG'] == 0].copy()\n",
    "    test_data['Gain_Corrected'] = rf_gap.predict(test_data[features])\n",
    "    # Final prediction: apply the multiplicative gain\n",
    "    test_data['Predicted_Final'] = test_data['Predicted_Symbolic'] * (1 + test_data['Gain_Corrected'])\n",
    "    \n",
    "    actual_values = test_data[target]\n",
    "    predicted_values = test_data['Predicted_Final']\n",
    "    \n",
    "    rmse, r2, mape = calculate_metrics(actual_values, predicted_values)\n",
    "    \n",
    "    ax.plot(test_data['TIMESTAMP'], actual_values, color='red', marker='o', linestyle='dashed', label='Actual (Gap)')\n",
    "    ax.plot(test_data['TIMESTAMP'], predicted_values, color='green', marker='o', linestyle='dashed', label='Predicted (Gap-filled)')\n",
    "    ax.axvspan(gap_start, gap_end, color='gray', alpha=0.2, label=f'{gap_days}-Day Gap')\n",
    "    \n",
    "    ax.set_title(f\"FNE | NEE Equation + RF Gain Correction | {gap_days}-Day Gap | R²: {round(r2, 2)}, RMSE: {round(rmse, 2)}, MAPE: {round(mape, 2)}\")\n",
    "    ax.set_xlabel(\"Timestamp\")\n",
    "    ax.set_ylabel(\"CO₂ Flux, micro-mol CO2 /m2\")\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "# Plot for different gap sizes\n",
    "for i, gap in enumerate(gap_sizes):\n",
    "    plot_gap(DF, gap, axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing NEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as jb\n",
    "units = jb.load('units.pkl')\n",
    "HE = pd.read_parquet('HurstEast.parquet')\n",
    "HW = pd.read_parquet('HurstWest.parquet')\n",
    "FNE = pd.read_parquet('FergusonNE.parquet')\n",
    "FSW = pd.read_parquet('FergusonSW.parquet')\n",
    "\n",
    "Names = ['HurstEast','HurstWest','FergusonNE','FergusonSW']\n",
    "DFS = [HE, HW, FNE, FSW]\n",
    "\n",
    "date_from = FNE['MIDPOINT_DATETIME'].iloc[0]\n",
    "date_to = FNE['MIDPOINT_DATETIME'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "# Define colors and labels for the 2 towers\n",
    "colors = ['b', 'r', 'b', 'r']  # First two for flux, last two for RECO\n",
    "tower_labels = ['FNE CO2', 'FSW CO2', 'FNE RECO', 'FSW RECO']\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "# Create 3x4 subplots (12 months)\n",
    "fig, axes = plt.subplots(3, 4, figsize=(10, 8), sharex=True, sharey=True)\n",
    "axes = axes.ravel()  # Flatten for easy iteration\n",
    "\n",
    "for month in range(1, 13):  # Loop through months (1 to 12)\n",
    "    ax = axes[month - 1]  # Get the subplot for the current month\n",
    "\n",
    "    for i, df in enumerate(DFS[2:4]):  # Loop through the 2 towers (FNE, FSW)\n",
    "        df['Hour'] = df['Hour'].astype(int)  # Ensure Hour is an integer\n",
    "        df['Month'] = df['Month'].astype(int)  # Ensure Month is an integer\n",
    "\n",
    "        # Group by Month and Hour to compute mean and standard deviation\n",
    "        agg_df = df.groupby(['Month', 'Hour'])['NET_CARBON_DIOXIDE_FLUX'].agg(['mean', 'std']).reset_index()\n",
    "        agg_df_reco = df.groupby(['Month', 'Hour'])['RECO'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "        # Filter for the current month\n",
    "        month_data_flux = agg_df[agg_df['Month'] == month]\n",
    "        month_data_reco = agg_df_reco[agg_df_reco['Month'] == month]\n",
    "\n",
    "        hours = month_data_flux['Hour']\n",
    "        mean_flux = month_data_flux['mean']\n",
    "        std_flux = month_data_flux['std']\n",
    "\n",
    "        hours_reco = month_data_reco['Hour']\n",
    "        mean_flux_reco = month_data_reco['mean']\n",
    "        std_flux_reco = month_data_reco['std']\n",
    "\n",
    "        # Plot mean flux\n",
    "        ax.plot(hours, mean_flux, color=colors[i], label=tower_labels[i])\n",
    "\n",
    "        # Fill ±1σ boundary for flux\n",
    "        ax.fill_between(hours, mean_flux - std_flux, mean_flux + std_flux, color=colors[i], alpha=0.2, label=\"_nolegend_\")\n",
    "\n",
    "        # Plot RECO\n",
    "        ax.plot(hours_reco, mean_flux_reco, color=colors[i+2], linestyle='dashed', label=tower_labels[i+2])\n",
    "\n",
    "        # Fill ±1σ boundary for RECO\n",
    "        ax.fill_between(hours_reco, mean_flux_reco - std_flux_reco, mean_flux_reco + std_flux_reco, \n",
    "                        color=colors[i+2], alpha=0.2, label=\"_nolegend_\")\n",
    "\n",
    "    # Titles and labels\n",
    "    ax.set_title(f'{month_names[month-1]}')\n",
    "    # ax.set_ylabel('Net CO₂ Flux (µmol/m²/s)')\n",
    "    ax.grid(True)\n",
    "\n",
    "fig.text(0.04, 0.5, 'Net CO₂ Flux (µmol/m²/s)', va='center', rotation='vertical', fontsize=12)\n",
    "\n",
    "fig.text(0.5, 0.04, 'Hour of the Day', ha='center', fontsize=12)\n",
    "\n",
    "# Set common x-label\n",
    "# for ax in axes[-4:]:  # Last row only\n",
    "#     ax.set_xlabel('Hour of the day')\n",
    "\n",
    "# Manually create legend to avoid shaded regions appearing\n",
    "legend_handles = [mlines.Line2D([], [], color=colors[i], linestyle='solid', label=tower_labels[i]) for i in range(2)] + \\\n",
    "                 [mlines.Line2D([], [], color=colors[i+2], linestyle='dashed', label=tower_labels[i+2]) for i in range(2)]\n",
    "\n",
    "fig.legend(handles=legend_handles, loc='upper right', fontsize=11)\n",
    "\n",
    "fig.suptitle('Mean Hourly Variation of Net CO₂ Flux & RECO for 2 Towers with ±1σ Boundary', fontsize=12)\n",
    "\n",
    "# plt.tight_layout(rect=[0, 0, 0.9, 0.97])  # Adjust layout for legend\n",
    "plt.tight_layout(rect=[0.05, 0.05, 0.9, 1])  # leave space for y-axis label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "# Define colors and labels for the 2 towers\n",
    "colors = ['b', 'r', 'b', 'r']  # First two for flux, last two for RECO\n",
    "tower_labels = ['HE CO2', 'HW CO2', 'HE RECO', 'HW RECO']\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "# Create 3x4 subplots (12 months)\n",
    "fig, axes = plt.subplots(3, 4, figsize=(10, 8), sharex=True, sharey=True)\n",
    "axes = axes.ravel()  # Flatten for easy iteration\n",
    "\n",
    "for month in range(1, 13):  # Loop through months (1 to 12)\n",
    "    ax = axes[month - 1]  # Get the subplot for the current month\n",
    "\n",
    "    for i, df in enumerate(DFS[0:2]):  # Loop through the 2 towers (FNE, FSW)\n",
    "        df['Hour'] = df['Hour'].astype(int)  # Ensure Hour is an integer\n",
    "        df['Month'] = df['Month'].astype(int)  # Ensure Month is an integer\n",
    "\n",
    "        # Group by Month and Hour to compute mean and standard deviation\n",
    "        agg_df = df.groupby(['Month', 'Hour'])['NET_CARBON_DIOXIDE_FLUX'].agg(['mean', 'std']).reset_index()\n",
    "        agg_df_reco = df.groupby(['Month', 'Hour'])['RECO'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "        # Filter for the current month\n",
    "        month_data_flux = agg_df[agg_df['Month'] == month]\n",
    "        month_data_reco = agg_df_reco[agg_df_reco['Month'] == month]\n",
    "\n",
    "        hours = month_data_flux['Hour']\n",
    "        mean_flux = month_data_flux['mean']\n",
    "        std_flux = month_data_flux['std']\n",
    "\n",
    "        hours_reco = month_data_reco['Hour']\n",
    "        mean_flux_reco = month_data_reco['mean']\n",
    "        std_flux_reco = month_data_reco['std']\n",
    "\n",
    "        # Plot mean flux\n",
    "        ax.plot(hours, mean_flux, color=colors[i], label=tower_labels[i])\n",
    "\n",
    "        # Fill ±1σ boundary for flux\n",
    "        ax.fill_between(hours, mean_flux - std_flux, mean_flux + std_flux, color=colors[i], alpha=0.2, label=\"_nolegend_\")\n",
    "\n",
    "        # Plot RECO\n",
    "        ax.plot(hours_reco, mean_flux_reco, color=colors[i+2], linestyle='dashed', label=tower_labels[i+2])\n",
    "\n",
    "        # Fill ±1σ boundary for RECO\n",
    "        ax.fill_between(hours_reco, mean_flux_reco - std_flux_reco, mean_flux_reco + std_flux_reco, \n",
    "                        color=colors[i+2], alpha=0.2, label=\"_nolegend_\")\n",
    "\n",
    "    # Titles and labels\n",
    "    ax.set_title(f'{month_names[month-1]}')\n",
    "    # ax.set_ylabel('Net CO₂ Flux (µmol/m²/s)')\n",
    "    ax.grid(True)\n",
    "\n",
    "fig.text(0.04, 0.5, 'Net CO₂ Flux (µmol/m²/s)', va='center', rotation='vertical', fontsize=12)\n",
    "\n",
    "fig.text(0.5, 0.04, 'Hour of the Day', ha='center', fontsize=12)\n",
    "\n",
    "# Set common x-label\n",
    "# for ax in axes[-4:]:  # Last row only\n",
    "#     ax.set_xlabel('Hour of the day')\n",
    "\n",
    "# Manually create legend to avoid shaded regions appearing\n",
    "legend_handles = [mlines.Line2D([], [], color=colors[i], linestyle='solid', label=tower_labels[i]) for i in range(2)] + \\\n",
    "                 [mlines.Line2D([], [], color=colors[i+2], linestyle='dashed', label=tower_labels[i+2]) for i in range(2)]\n",
    "\n",
    "fig.legend(handles=legend_handles, loc='upper right', fontsize=11)\n",
    "\n",
    "fig.suptitle('Mean Hourly Variation of Net CO₂ Flux & RECO for 2 Towers with ±1σ Boundary', fontsize=12)\n",
    "\n",
    "# plt.tight_layout(rect=[0, 0, 0.9, 0.97])  # Adjust layout for legend\n",
    "plt.tight_layout(rect=[0.05, 0.05, 0.9, 1])  # leave space for y-axis label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8), sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "# Find global min and max for color scaling\n",
    "vmin = min(df['NET_CARBON_DIOXIDE_FLUX'].min() for df in DFS)\n",
    "vmax = max(df['NET_CARBON_DIOXIDE_FLUX'].max() for df in DFS)\n",
    "norm = mpl.colors.Normalize(vmin=vmin, vmax=-vmin)\n",
    "\n",
    "# Create scatter plots\n",
    "mappables = []\n",
    "for i, (name, df) in enumerate(zip(Names, DFS)):\n",
    "    sc = ax[i].scatter(df['Time'], df['INCOMING_SHORTWAVE_RADIATION'], \n",
    "                       c=df['NET_CARBON_DIOXIDE_FLUX'], cmap='seismic', norm=norm)\n",
    "    mappables.append(sc)\n",
    "    \n",
    "    ax[i].set_title(name)\n",
    "    ax[i].set_xticks(df['Time'][::1600])\n",
    "    ax[i].set_xticklabels(df['Month'][::1600], rotation=0)\n",
    "\n",
    "# Shared x and y axis labels\n",
    "fig.text(0.5, 0.04, 'Month', ha='center', fontsize=12)\n",
    "fig.text(0.04, 0.5, 'Incoming Shortwave Radiation (W/m²)', va='center', rotation='vertical', fontsize=12)\n",
    "\n",
    "# Shared colorbar\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "fig.colorbar(mappables[0], cax=cbar_ax, label='Net CO₂ Flux (µmol/m²/s)')\n",
    "\n",
    "# Super title\n",
    "fig.suptitle(f'Incoming Shortwave Radiation vs Month Sep-2023 -> Dec-2024', fontsize=14)\n",
    "\n",
    "plt.tight_layout(rect=[0.06, 0.06, 0.9, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nightime = FSW[FSW['INCOMING_SHORTWAVE_RADIATION']<0][FSW['NET_CARBON_DIOXIDE_FLUX_FLAG']==0]\n",
    "plt.scatter(Nightime['AIR_TEMPERATURE'],Nightime['NET_CARBON_DIOXIDE_FLUX'],c = Nightime['Month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
